# Phase 2 μ™„λ£ λ³΄κ³ μ„

## β… μ™„λ£ ν•­λ©

### 1. ν•µμ‹¬ μ ν‹Έλ¦¬ν‹° κµ¬ν„
- β… `src/utils.py`: μ„¤μ • λ΅λ”©, μ‹λ“ μ„¤μ •, λ””λ°”μ΄μ¤ κ΄€λ¦¬, GPU λ©”λ¨λ¦¬ λ¨λ‹ν„°λ§
- β… μ¬ν„μ„±, λ””λ°”μ΄μ¤ κ΄€λ¦¬, νλΌλ―Έν„° μΉ΄μ΄ν…, GPU λ©”λ¨λ¦¬ μ¶”μ  κΈ°λ¥

### 2. λ°μ΄ν„°μ…‹ ν΄λμ¤ κµ¬ν„
- β… `src/dataset.py`: PyTorch Dataset λ° DataLoader κµ¬ν„
- β… InstructionDataset, SimpleTextDataset ν΄λμ¤
- β… μλ™ ν† ν¬λ‚μ΄μ§•, ν¨λ”©, λΌλ²¨ λ§μ¤ν‚Ή μ²λ¦¬

### 3. λ°μ΄ν„° μ¤€λΉ„ μ¤ν¬λ¦½νΈ
- β… `data/prepare_dataset.py`: λ°μ΄ν„°μ…‹ λ‹¤μ΄λ΅λ“ λ° μ „μ²λ¦¬
- β… 1,000κ° μƒν” λ°μ΄ν„° μƒμ„± (Train: 800, Val: 100, Test: 100)
- β… 10κ° ν•κµ­μ–΄ instruction ν…ν”λ¦Ώ κΈ°λ°

### 4. ν…μ¤νΈ μ¤ν¬λ¦½νΈ
- β… `tests/test_dataset.py`: λ¨λ“  ν…μ¤νΈ ν†µκ³Ό
- β… ν† ν¬λ‚μ΄μ €, λ°μ΄ν„°μ…‹, DataLoader κ²€μ¦ μ™„λ£

---

## π“ ν…μ¤νΈ κ²°κ³Ό

```
β… Tokenizer: GPT2TokenizerFast (50,257 vocab)
β… Train dataset: 800 samples
β… Val dataset: 100 samples
β… Train batches: 400 (batch_size=2)
β… Val batches: 50 (batch_size=2)
β… Average padding ratio: ~35%
```

---

## π€ λ‹¤μ λ‹¨κ³„: Phase 3

Phase 3μ—μ„λ” λ¨λΈ λ° LoRA μ„¤μ •μ„ κµ¬ν„ν•©λ‹λ‹¤:

1. **λ¨λΈ λ΅λ”©** (`src/model.py`)
   - GPT-2 λ¨λΈ λ΅λ”©
   - LoRA μ„¤μ • λ° μ μ©
   - νλΌλ―Έν„° λ™κ²° λ° ν•™μµ κ°€λ¥ νλΌλ―Έν„° μ„¤μ •

2. **μμƒ μ‚°μ¶λ¬Ό**
   - `src/model.py`
   - λ¨λΈ λ΅λ”© κ²€μ¦ μ¤ν¬λ¦½νΈ
   - LoRA μ μ© μ „ν›„ νλΌλ―Έν„° λΉ„κµ

**μμƒ μ†μ” μ‹κ°„**: 45λ¶„

---

## β… Phase 2 μ™„λ£!

λ°μ΄ν„° μ¤€λΉ„ λ° μ „μ²λ¦¬ νμ΄ν”„λΌμΈμ΄ μ™„λ£λμ—μµλ‹λ‹¤! π‰
