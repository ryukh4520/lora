# LoRA μ‹κ°μ  μ„¤λ…

## π¨ LoRA λ™μ‘ μ›λ¦¬ μ‹κ°ν™”

### 1. Full Fine-tuning vs LoRA

```
β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
β”‚                    Full Fine-tuning                          β”‚
β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”

Input (x)
   β”‚
   β–Ό
β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
β”‚  Weight Matrix   β”‚  W β β„^(768Γ—768)
β”‚  (Trainable)     β”‚  β† λ¨λ“  589,824 νλΌλ―Έν„° ν•™μµ!
β”‚  W β†’ W'          β”‚
β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
   β”‚
   β–Ό
Output (h)

λ©”λ¨λ¦¬: 589,824 Γ— 4 bytes = 2.36 MB (νλΌλ―Έν„°λ§)
        + Gradient: 2.36 MB
        + Optimizer: 4.72 MB (Adam)
        = μ΄ 9.44 MB per layer


β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
β”‚                         LoRA                                 β”‚
β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”

Input (x)
   β”‚
   β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
   β”‚                 β”‚
   β–Ό                 β–Ό
β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”    β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
β”‚ Weight W β”‚    β”‚ LoRA A   β”‚  A β β„^(8Γ—768)
β”‚ (Frozen) β”‚    β”‚(Trainableβ”‚  = 6,144 params
β”‚    β„οΈ    β”‚    β”‚    π”¥    β”‚
β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”    β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
   β”‚                 β”‚
   β”‚                 β–Ό
   β”‚            β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
   β”‚            β”‚ LoRA B   β”‚  B β β„^(768Γ—8)
   β”‚            β”‚(Trainableβ”‚  = 6,144 params
   β”‚            β”‚    π”¥    β”‚
   β”‚            β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
   β”‚                 β”‚
   β”‚                 β”‚ Γ— (alpha/r)
   β”‚                 β”‚
   β””β”€β”€β”€β”€β”€β”€β”€β”€β”¬β”€β”€β”€β”€β”€β”€β”€β”€β”
            β”‚ (Add)
            β–Ό
        Output (h)

λ©”λ¨λ¦¬: 12,288 Γ— 4 bytes = 49 KB (νλΌλ―Έν„°λ§)
        + Gradient: 49 KB
        + Optimizer: 98 KB (Adam)
        = μ΄ 196 KB per layer

μ μ•½: 9.44 MB β†’ 0.196 MB (98% κ°μ†!)
```

---

### 2. LoRA ν–‰λ ¬ λ¶„ν•΄ μƒμ„Έ

```
μ›λ κ°€μ¤‘μΉ μ—…λ°μ΄νΈ:
β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
β”‚                                         β”‚
β”‚         W' = W + Ξ”W                    β”‚
β”‚                                         β”‚
β”‚  Ξ”W β β„^(768Γ—768) = 589,824 params    β”‚
β”‚                                         β”‚
β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”


LoRA λ¶„ν•΄:
β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
β”‚                                         β”‚
β”‚         Ξ”W = B Γ— A                     β”‚
β”‚                                         β”‚
β”‚  B β β„^(768Γ—8)  = 6,144 params        β”‚
β”‚  A β β„^(8Γ—768)  = 6,144 params        β”‚
β”‚  Total          = 12,288 params        β”‚
β”‚                                         β”‚
β”‚  Compression: 589,824 β†’ 12,288         β”‚
β”‚  Ratio: 98% reduction!                 β”‚
β”‚                                         β”‚
β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”


μ‹κ°ν™”:
        768
    β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
    β”‚         β”‚
768 β”‚   Ξ”W    β”‚  589,824 params
    β”‚         β”‚
    β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
         β†“
    λ¶„ν•΄ (Decompose)
         β†“
    β”β”€β”€β”€β”   β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
    β”‚   β”‚   β”‚         β”‚
768 β”‚ B β”‚ Γ— β”‚    A    β”‚  12,288 params
    β”‚   β”‚ 8 β”‚         β”‚
    β””β”€β”€β”€β”   β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
      8         768
```

---

### 3. Forward Pass μƒμ„Έ

```
β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
β”‚                  Forward Computation                      β”‚
β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”

Input: x β β„^(batch_size Γ— 768)

Step 1: μ›λ λ μ΄μ–΄ κ³„μ‚° (Frozen)
β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
β”‚  h_base = W Β· x                     β”‚
β”‚                                     β”‚
β”‚  W: 768Γ—768 (Frozen β„οΈ)            β”‚
β”‚  x: batchΓ—768                       β”‚
β”‚  h_base: batchΓ—768                  β”‚
β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”

Step 2: LoRA κ³„μ‚° (Trainable)
β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
β”‚  temp = A Β· x                       β”‚
β”‚  A: 8Γ—768 (Trainable π”¥)           β”‚
β”‚  temp: batchΓ—8                      β”‚
β”‚                                     β”‚
β”‚  h_lora = B Β· temp                  β”‚
β”‚  B: 768Γ—8 (Trainable π”¥)           β”‚
β”‚  h_lora: batchΓ—768                  β”‚
β”‚                                     β”‚
β”‚  h_lora = h_lora Γ— (alpha/r)       β”‚
β”‚  scaling: 16/8 = 2.0                β”‚
β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”

Step 3: κ²°ν•©
β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
β”‚  h = h_base + h_lora                β”‚
β”‚                                     β”‚
β”‚  h: batchΓ—768                       β”‚
β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”

μ‹κ°ν™”:
    x (batchΓ—768)
    β”‚
    β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
    β”‚                  β”‚
    β–Ό                  β–Ό
  WΒ·x              AΒ·x (batchΓ—8)
(batchΓ—768)          β”‚
    β”‚                β–Ό
    β”‚              BΒ·(AΒ·x) (batchΓ—768)
    β”‚                β”‚
    β”‚                β”‚ Γ— 2.0
    β”‚                β”‚
    β””β”€β”€β”€β”€β”€β”€ + β”€β”€β”€β”€β”€β”€β”€β”
           β”‚
           β–Ό
      h (batchΓ—768)
```

---

### 4. GPT-2 Attention Layer κµ¬μ΅°

```
β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
β”‚              GPT-2 Transformer Layer                        β”‚
β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”

Input Embedding (batch Γ— seq_len Γ— 768)
    β”‚
    β–Ό
β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
β”‚         Multi-Head Attention             β”‚
β”‚                                          β”‚
β”‚  β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”    β”‚
β”‚  β”‚  c_attn (QKV projection)       β”‚    β”‚
β”‚  β”‚  W: 768 β†’ 2304 (Q,K,V)        β”‚    β”‚
β”‚  β”‚  LoRA: r=8                     β”‚    β”‚
β”‚  β”‚  A: 8Γ—768, B: 2304Γ—8          β”‚    β”‚
β”‚  β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”    β”‚
β”‚           β”‚                             β”‚
β”‚           β–Ό                             β”‚
β”‚  β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”    β”‚
β”‚  β”‚  Attention Computation         β”‚    β”‚
β”‚  β”‚  (No LoRA here)                β”‚    β”‚
β”‚  β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”    β”‚
β”‚           β”‚                             β”‚
β”‚           β–Ό                             β”‚
β”‚  β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”    β”‚
β”‚  β”‚  c_proj (Output projection)    β”‚    β”‚
β”‚  β”‚  W: 768 β†’ 768                  β”‚    β”‚
β”‚  β”‚  LoRA: r=8                     β”‚    β”‚
β”‚  β”‚  A: 8Γ—768, B: 768Γ—8           β”‚    β”‚
β”‚  β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”    β”‚
β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
    β”‚
    β–Ό
β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
β”‚              Feed Forward                β”‚
β”‚  (No LoRA in our config)                β”‚
β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
    β”‚
    β–Ό
Output (batch Γ— seq_len Γ— 768)


LoRA μ μ© μ„μΉ:
β… c_attn:  QKV projection (2304Γ—768)
β… c_proj:  Output projection (768Γ—768)
β FFN:     Feed-forward (μ„ νƒμ )
```

---

### 5. νλΌλ―Έν„° μ λΉ„κµ (GPT-2 Small)

```
β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
β”‚           Full Fine-tuning vs LoRA (r=8)                    β”‚
β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”

Layer: c_attn (768 β†’ 2304)
β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
β”‚ Full Fine-tuning                     β”‚
β”‚ β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β” β”‚ 1,769,472 params
β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”

β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
β”‚ LoRA (r=8)                           β”‚
β”‚ β–“β–“                                   β”‚ 24,576 params (1.4%)
β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”


Layer: c_proj (768 β†’ 768)
β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
β”‚ Full Fine-tuning                     β”‚
β”‚ β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β” β”‚ 589,824 params
β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”

β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
β”‚ LoRA (r=8)                           β”‚
β”‚ β–“β–“                                   β”‚ 12,288 params (2.1%)
β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”


μ „μ²΄ λ¨λΈ (12 layers)
β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
β”‚ Full Fine-tuning                     β”‚
β”‚ β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β” β”‚ 124,439,808 params
β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”

β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
β”‚ LoRA (r=8)                           β”‚
β”‚ β–“                                    β”‚ 811,008 params (0.65%)
β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”

μ μ•½: 99.35% νλΌλ―Έν„° λ™κ²°!
```

---

### 6. Rankμ— λ”°λ¥Έ ν‘ν„λ ¥ vs ν¨μ¨μ„±

```
β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
β”‚              Rank vs Performance Trade-off                  β”‚
β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”

r=1:  λ§¤μ° μ ν•μ 
β”β”€β”
β”‚β–β”‚  ν‘ν„λ ¥: β­
β””β”€β”  νλΌλ―Έν„°: μµμ†

r=4:  κ°„λ‹¨ν• νƒμ¤ν¬
β”β”€β”€β”€β”€β”
β”‚β–β–β–β–β”‚  ν‘ν„λ ¥: β­β­β­
β””β”€β”€β”€β”€β”  νλΌλ―Έν„°: μ μ

r=8:  μΌλ°μ  (μ°λ¦¬ ν”„λ΅μ νΈ)
β”β”€β”€β”€β”€β”€β”€β”€β”€β”
β”‚β–β–β–β–β–β–β–β–β”‚  ν‘ν„λ ¥: β­β­β­β­
β””β”€β”€β”€β”€β”€β”€β”€β”€β”  νλΌλ―Έν„°: λ³΄ν†µ

r=16: λ³µμ΅ν• νƒμ¤ν¬
β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
β”‚β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β”‚  ν‘ν„λ ¥: β­β­β­β­β­
β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”  νλΌλ―Έν„°: λ§μ

r=32: λ§¤μ° λ³µμ΅ν• νƒμ¤ν¬
β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
β”‚β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β”‚  ν‘ν„λ ¥: β­β­β­β­β­
β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”  νλΌλ―Έν„°: λ§¤μ° λ§μ


νλΌλ―Έν„° μ (c_attn + c_proj per layer):
r=4:   18,432 params
r=8:   36,864 params  β† μ°λ¦¬ μ„ νƒ
r=16:  73,728 params
r=32: 147,456 params

κ¶μ¥:
- κ°„λ‹¨ν• QA: r=4-8
- μΌλ°μ  νƒμ¤ν¬: r=8-16
- λ³µμ΅ν• μƒμ„±: r=16-32
```

---

### 7. ν•™μµ κ³Όμ • μ‹κ°ν™”

```
β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
β”‚                    Training Process                         β”‚
β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”

Epoch 1: μ΄κΈ° μƒνƒ
β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
β”‚ W (Frozen)    B (Zero)    A (Random) β”‚
β”‚ β”β”β”β”β”β”β”β”β”    β–‘β–‘β–‘β–‘β–‘β–‘β–‘β–‘    β–’β–’β–’β–’β–’β–’β–’β–’  β”‚
β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
Loss: 2.66, Perplexity: 9.08

Epoch 5: ν•™μµ μ¤‘
β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
β”‚ W (Frozen)    B (Learning) A (Learning)β”‚
β”‚ β”β”β”β”β”β”β”β”β”    β–“β–“β–“β–“β–“β–“β–“β–“    β–“β–“β–“β–“β–“β–“β–“β–“  β”‚
β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
Loss: 1.14, Perplexity: 3.12

Epoch 10: μλ ΄ μ¤‘
β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
β”‚ W (Frozen)    B (Learned)  A (Learned) β”‚
β”‚ β”β”β”β”β”β”β”β”β”    β–β–β–β–β–β–β–β–    β–β–β–β–β–β–β–β–  β”‚
β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
Loss: 0.59, Perplexity: 1.67

Epoch 20: κ±°μ μ™„λ²½
β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
β”‚ W (Frozen)    B (Optimized) A (Optimized)β”‚
β”‚ β”β”β”β”β”β”β”β”β”    β–β–β–β–β–β–β–β–    β–β–β–β–β–β–β–β–  β”‚
β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
Loss: 0.32, Perplexity: 1.05

Wλ” ν•­μƒ λ™κ²° β„οΈ
B, Aλ§ ν•™μµ π”¥
```

---

### 8. λ©”λ¨λ¦¬ μ‚¬μ©λ‰ λΉ„κµ

```
β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
β”‚              Memory Usage (Training)                        β”‚
β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”

Full Fine-tuning:
β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
β”‚ Model Parameters:        500 MB                         β”‚
β”‚ Gradients:               500 MB                         β”‚
β”‚ Optimizer States (Adam): 1000 MB                        β”‚
β”‚ Activations:             500 MB                         β”‚
β”‚ β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€ β”‚
β”‚ Total:                   2500 MB (2.5 GB)               β”‚
β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”

LoRA (r=8):
β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
β”‚ Model Parameters:        500 MB (Frozen, no grad)      β”‚
β”‚ LoRA Parameters:         3.2 MB                         β”‚
β”‚ Gradients:               3.2 MB                         β”‚
β”‚ Optimizer States (Adam): 6.4 MB                         β”‚
β”‚ Activations:             500 MB                         β”‚
β”‚ β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€ β”‚
β”‚ Total:                   1013 MB (1.0 GB)               β”‚
β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”

μ μ•½: 2.5 GB β†’ 1.0 GB (60% κ°μ†)

μ‹¤μ  μΈ΅μ • (μ°λ¦¬ ν”„λ΅μ νΈ):
- VRAM μ‚¬μ©: 0.50 GB / 8.00 GB
- μ—¬μ : 7.50 GB (93.75%)
```

---

### 9. μ²΄ν¬ν¬μΈνΈ ν¬κΈ° λΉ„κµ

```
β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
β”‚              Checkpoint Size                                β”‚
β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”

Full Model:
β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
β”‚ β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β”‚
β”‚                     ~500 MB                              β”‚
β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”

LoRA Adapter:
β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
β”‚ β–                                                        β”‚
β”‚                     ~9.4 MB                              β”‚
β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”

λΉ„μ¨: 9.4 MB / 500 MB = 1.88%
μ μ•½: 98.12%

λ‹¤μ¤‘ νƒμ¤ν¬ μ‹λ‚λ¦¬μ¤:
β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
β”‚ Base Model (1κ°):        500 MB                         β”‚
β”‚ Task 1 LoRA:             9.4 MB                         β”‚
β”‚ Task 2 LoRA:             9.4 MB                         β”‚
β”‚ Task 3 LoRA:             9.4 MB                         β”‚
β”‚ Task 4 LoRA:             9.4 MB                         β”‚
β”‚ Task 5 LoRA:             9.4 MB                         β”‚
β”‚ β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€ β”‚
β”‚ Total:                   547 MB                         β”‚
β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”

vs Full Models:
500 MB Γ— 6 = 3000 MB

μ μ•½: 3000 MB β†’ 547 MB (82% κ°μ†)
```

---

## π― ν•µμ‹¬ μ”μ•½

### LoRAμ λ§λ²• β¨

```
ν° ν–‰λ ¬ Ξ”W (768Γ—768 = 589,824 params)
         β†“
    λ¶„ν•΄ (Decompose)
         β†“
μ‘μ€ ν–‰λ ¬ B (768Γ—8) + A (8Γ—768)
         β†“
    12,288 params (98% μ μ•½!)
```

### μ‹¤μ  ν¨κ³Ό (μ°λ¦¬ ν”„λ΅μ νΈ)

```
β… νλΌλ―Έν„°: 124M β†’ 0.8M (99.4% λ™κ²°)
β… μ²΄ν¬ν¬μΈνΈ: 500MB β†’ 9.4MB (98% μ μ•½)
β… VRAM: 2.5GB β†’ 0.5GB (80% μ μ•½)
β… μ„±λ¥: Perplexity 88% κ°μ†!
```

---

μ΄μ  LoRAκ°€ μ–΄λ–»κ² λ™μ‘ν•λ”μ§€ μ™„λ²½ν μ΄ν•΄ν•μ…¨μ„ κ²ƒμ…λ‹λ‹¤! π€
