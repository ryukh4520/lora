# LoRA Target Module ì„ ì • ì™„ì „ ê°€ì´ë“œ

## ğŸ¯ ëª©ì°¨
1. [ëª¨ë¸ êµ¬ì¡° ì´í•´í•˜ê¸°](#ëª¨ë¸-êµ¬ì¡°-ì´í•´í•˜ê¸°)
2. [Target Module ì°¾ê¸°](#target-module-ì°¾ê¸°)
3. [ì„ ì • ê¸°ì¤€ê³¼ ì „ëµ](#ì„ ì •-ê¸°ì¤€ê³¼-ì „ëµ)
4. [ì‹¤ì „ ì„ ì • í”„ë¡œì„¸ìŠ¤](#ì‹¤ì „-ì„ ì •-í”„ë¡œì„¸ìŠ¤)
5. [ëª¨ë¸ë³„ ê¶Œì¥ ì„¤ì •](#ëª¨ë¸ë³„-ê¶Œì¥-ì„¤ì •)

---

## ğŸ—ï¸ 1. ëª¨ë¸ êµ¬ì¡° ì´í•´í•˜ê¸°

### 1.1 Transformer ê¸°ë³¸ êµ¬ì¡°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Transformer Layer                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Input Embedding
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Multi-Head Self-Attention          â”‚
â”‚                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Query Projection (Q)          â”‚ â”‚ â† LoRA ì ìš© ê°€ëŠ¥
â”‚  â”‚ W_q: (d_model, d_model)       â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Key Projection (K)            â”‚ â”‚ â† LoRA ì ìš© ê°€ëŠ¥
â”‚  â”‚ W_k: (d_model, d_model)       â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Value Projection (V)          â”‚ â”‚ â† LoRA ì ìš© ê°€ëŠ¥
â”‚  â”‚ W_v: (d_model, d_model)       â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Attention Computation         â”‚ â”‚
â”‚  â”‚ Softmax(QK^T/âˆšd)V            â”‚ â”‚ (LoRA ì ìš© ì•ˆí•¨)
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Output Projection (O)         â”‚ â”‚ â† LoRA ì ìš© ê°€ëŠ¥
â”‚  â”‚ W_o: (d_model, d_model)       â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Feed-Forward Network (FFN)         â”‚
â”‚                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Up Projection                 â”‚ â”‚ â† LoRA ì ìš© ê°€ëŠ¥
â”‚  â”‚ W_up: (d_model, d_ff)         â”‚ â”‚
â”‚  â”‚ (ì˜ˆ: 768 â†’ 3072)              â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Activation (GELU, ReLU)       â”‚ â”‚ (LoRA ì ìš© ì•ˆí•¨)
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Down Projection               â”‚ â”‚ â† LoRA ì ìš© ê°€ëŠ¥
â”‚  â”‚ W_down: (d_ff, d_model)       â”‚ â”‚
â”‚  â”‚ (ì˜ˆ: 3072 â†’ 768)              â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Output
```

**LoRA ì ìš© ê°€ëŠ¥ ë ˆì´ì–´**:
```
âœ… Linear layers (nn.Linear)
âŒ Activation functions
âŒ LayerNorm
âŒ Dropout
âŒ Embedding layers (ì„ íƒì )
```

---

### 1.2 ì™œ Linear Layerë§Œ ì ìš©í•˜ëŠ”ê°€?

```python
# Linear layerì˜ íŠ¹ì§•

class Linear(nn.Module):
    def __init__(self, in_features, out_features):
        self.weight = Parameter(torch.randn(out_features, in_features))
        self.bias = Parameter(torch.randn(out_features))
    
    def forward(self, x):
        return x @ self.weight.T + self.bias
        #      â†‘
        # í–‰ë ¬ ê³± â†’ LoRA ì ìš© ê°€ëŠ¥!

# LoRA ì ìš©
output = x @ W.T + (x @ A.T) @ B.T * (alpha/r)
         â†‘        â†‘
      ì›ë˜ ê°€ì¤‘ì¹˜  LoRA ì¶”ê°€
```

**ì´ìœ **:
```
1. í–‰ë ¬ ê³± ì—°ì‚°
   â†’ Low-rank decomposition ê°€ëŠ¥

2. ëŒ€ë¶€ë¶„ì˜ íŒŒë¼ë¯¸í„°
   â†’ Transformerì˜ 90%+ íŒŒë¼ë¯¸í„°ê°€ Linear

3. í•™ìŠµ íš¨ê³¼
   â†’ Linear layerê°€ í‘œí˜„ë ¥ì˜ í•µì‹¬
```

---

## ğŸ” 2. Target Module ì°¾ê¸°

### 2.1 ëª¨ë¸ êµ¬ì¡° íƒìƒ‰

#### **Step 1: ëª¨ë¸ ë¡œë“œ**

```python
from transformers import AutoModelForCausalLM

# ëª¨ë¸ ë¡œë“œ
model = AutoModelForCausalLM.from_pretrained("gpt2")

print(model)
```

**ì¶œë ¥ ì˜ˆì‹œ (GPT-2)**:
```
GPT2LMHeadModel(
  (transformer): GPT2Model(
    (wte): Embedding(50257, 768)
    (wpe): Embedding(1024, 768)
    (drop): Dropout(p=0.1)
    (h): ModuleList(
      (0-11): 12 x GPT2Block(
        (ln_1): LayerNorm((768,))
        (attn): GPT2Attention(
          (c_attn): Conv1D()      â† ì´ê²ƒ!
          (c_proj): Conv1D()      â† ì´ê²ƒ!
          (attn_dropout): Dropout(p=0.1)
          (resid_dropout): Dropout(p=0.1)
        )
        (ln_2): LayerNorm((768,))
        (mlp): GPT2MLP(
          (c_fc): Conv1D()        â† ì´ê²ƒ!
          (c_proj): Conv1D()      â† ì´ê²ƒ!
          (act): NewGELUActivation()
          (dropout): Dropout(p=0.1)
        )
      )
    )
    (ln_f): LayerNorm((768,))
  )
  (lm_head): Linear(in_features=768, out_features=50257)
)
```

---

#### **Step 2: Linear Layer ì°¾ê¸°**

```python
def find_linear_layers(model):
    """ëª¨ë¸ì˜ ëª¨ë“  Linear ë ˆì´ì–´ ì°¾ê¸°"""
    linear_layers = {}
    
    for name, module in model.named_modules():
        # Linear ë˜ëŠ” Conv1D (GPT-2ì˜ ê²½ìš°)
        if isinstance(module, (torch.nn.Linear, torch.nn.Conv1D)):
            # í¬ê¸° ì •ë³´
            if hasattr(module, 'weight'):
                shape = module.weight.shape
                linear_layers[name] = {
                    'type': module.__class__.__name__,
                    'shape': shape,
                    'params': shape[0] * shape[1]
                }
    
    return linear_layers

# ì‚¬ìš©
layers = find_linear_layers(model)
for name, info in layers.items():
    print(f"{name}: {info['shape']} ({info['params']:,} params)")
```

**ì¶œë ¥ ì˜ˆì‹œ (GPT-2)**:
```
transformer.h.0.attn.c_attn: (2304, 768) (1,769,472 params)
transformer.h.0.attn.c_proj: (768, 768) (589,824 params)
transformer.h.0.mlp.c_fc: (3072, 768) (2,359,296 params)
transformer.h.0.mlp.c_proj: (768, 3072) (2,359,296 params)
...
(12 layers ë°˜ë³µ)
lm_head: (50257, 768) (38,597,376 params)
```

---

#### **Step 3: ëª¨ë“ˆ ì´ë¦„ íŒ¨í„´ íŒŒì•…**

```python
def analyze_module_patterns(model):
    """ëª¨ë“ˆ ì´ë¦„ íŒ¨í„´ ë¶„ì„"""
    patterns = {}
    
    for name, module in model.named_modules():
        if isinstance(module, (torch.nn.Linear, torch.nn.Conv1D)):
            # ë§ˆì§€ë§‰ ë¶€ë¶„ ì¶”ì¶œ (ì˜ˆ: "c_attn")
            module_name = name.split('.')[-1]
            
            if module_name not in patterns:
                patterns[module_name] = []
            patterns[module_name].append(name)
    
    return patterns

# ì‚¬ìš©
patterns = analyze_module_patterns(model)
for pattern, occurrences in patterns.items():
    print(f"\n{pattern}: {len(occurrences)}ê°œ")
    print(f"  ì˜ˆì‹œ: {occurrences[0]}")
```

**ì¶œë ¥ ì˜ˆì‹œ (GPT-2)**:
```
c_attn: 12ê°œ
  ì˜ˆì‹œ: transformer.h.0.attn.c_attn

c_proj: 24ê°œ (attn 12ê°œ + mlp 12ê°œ)
  ì˜ˆì‹œ: transformer.h.0.attn.c_proj
  ì˜ˆì‹œ: transformer.h.0.mlp.c_proj

c_fc: 12ê°œ
  ì˜ˆì‹œ: transformer.h.0.mlp.c_fc

lm_head: 1ê°œ
  ì˜ˆì‹œ: lm_head
```

---

### 2.2 ëª¨ë¸ë³„ ëª¨ë“ˆ ì´ë¦„

#### **GPT-2**

```python
# Attention
"c_attn"   # QKV projection (í†µí•©)
"c_proj"   # Output projection

# MLP
"c_fc"     # Up projection
"c_proj"   # Down projection (ì´ë¦„ ì¤‘ë³µ!)

# ì£¼ì˜: c_projê°€ attnê³¼ mlpì— ëª¨ë‘ ìˆìŒ!
```

---

#### **LLaMA / Mistral**

```python
# Attention
"q_proj"   # Query projection
"k_proj"   # Key projection
"v_proj"   # Value projection
"o_proj"   # Output projection

# MLP
"gate_proj"  # Gate projection
"up_proj"    # Up projection
"down_proj"  # Down projection
```

---

#### **BERT**

```python
# Attention
"query"    # Query projection
"key"      # Key projection
"value"    # Value projection

# Output
"dense"    # Output projection (ì—¬ëŸ¬ ê³³ì— ìˆìŒ)

# MLP
"intermediate.dense"  # Up projection
"output.dense"        # Down projection
```

---

## ğŸ“‹ 3. ì„ ì • ê¸°ì¤€ê³¼ ì „ëµ

### 3.1 ì„ ì • ê¸°ì¤€

#### **ê¸°ì¤€ 1: íŒŒë¼ë¯¸í„° ìˆ˜**

```python
# íŒŒë¼ë¯¸í„°ê°€ ë§ì€ ë ˆì´ì–´ ìš°ì„ 

GPT-2 ì˜ˆì‹œ:
c_attn:  1,769,472 params  â† ê°€ì¥ í¼
mlp.c_fc: 2,359,296 params â† ê°€ì¥ í¼
mlp.c_proj: 2,359,296 params
attn.c_proj: 589,824 params

â†’ í° ë ˆì´ì–´ì— LoRA ì ìš© ì‹œ íš¨ê³¼ì 
```

---

#### **ê¸°ì¤€ 2: íƒœìŠ¤í¬ ê´€ë ¨ì„±**

```python
# íƒœìŠ¤í¬ì— ì¤‘ìš”í•œ ë ˆì´ì–´ ìš°ì„ 

QA, ë¶„ë¥˜:
â†’ Attention layers (Q, K, V, O)
â†’ ë¬¸ë§¥ ì´í•´ê°€ ì¤‘ìš”

ìƒì„±, ìš”ì•½:
â†’ Attention + MLP
â†’ í‘œí˜„ë ¥ì´ ì¤‘ìš”

ë²ˆì—­:
â†’ Attention layers
â†’ ì •ë ¬(alignment)ì´ ì¤‘ìš”
```

---

#### **ê¸°ì¤€ 3: ë©”ëª¨ë¦¬ ì œì•½**

```python
# ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì ˆ

4GB VRAM:
â†’ Attentionë§Œ (Q, Vë§Œ ë˜ëŠ” Q, K, V, O)

8GB VRAM:
â†’ Attention ì „ì²´ (Q, K, V, O)

16GB+ VRAM:
â†’ Attention + MLP
```

---

### 3.2 ì„ ì • ì „ëµ

#### **ì „ëµ 1: Attention Only (ê¸°ë³¸, ê¶Œì¥)**

```python
# GPT-2
target_modules = ["c_attn", "c_proj"]

# LLaMA
target_modules = ["q_proj", "k_proj", "v_proj", "o_proj"]

# BERT
target_modules = ["query", "key", "value"]

ì¥ì :
âœ… íŒŒë¼ë¯¸í„° íš¨ìœ¨ì 
âœ… ëŒ€ë¶€ë¶„ì˜ ê²½ìš° ì¶©ë¶„
âœ… ë¹ ë¥¸ í•™ìŠµ
âœ… ë©”ëª¨ë¦¬ ì ˆì•½

ì‚¬ìš©:
- ê°„ë‹¨í•œ QA
- ë¶„ë¥˜
- ìš”ì•½
- ìš°ë¦¬ í”„ë¡œì íŠ¸ âœ…
```

---

#### **ì „ëµ 2: Attention + MLP (ë†’ì€ ì„±ëŠ¥)**

```python
# GPT-2
target_modules = ["c_attn", "c_proj", "c_fc"]

# LLaMA
target_modules = [
    "q_proj", "k_proj", "v_proj", "o_proj",
    "gate_proj", "up_proj", "down_proj"
]

ì¥ì :
âœ… ë†’ì€ í‘œí˜„ë ¥
âœ… ë³µì¡í•œ íƒœìŠ¤í¬ ëŒ€ì‘

ë‹¨ì :
âš ï¸ íŒŒë¼ë¯¸í„° 2-3ë°° ì¦ê°€
âš ï¸ ë©”ëª¨ë¦¬ ì‚¬ìš© ì¦ê°€
âš ï¸ í•™ìŠµ ì‹œê°„ ì¦ê°€

ì‚¬ìš©:
- ë³µì¡í•œ ìƒì„±
- ì°½ì˜ì  ê¸€ì“°ê¸°
- ì „ë¬¸ ë„ë©”ì¸
```

---

#### **ì „ëµ 3: Query + Value Only (íš¨ìœ¨ì )**

```python
# LLaMA
target_modules = ["q_proj", "v_proj"]

ì¥ì :
âœ… íŒŒë¼ë¯¸í„° ì ˆì•½ (50%)
âœ… ì—¬ì „íˆ íš¨ê³¼ì 

ì´ë¡ :
- Query: "ë¬´ì—‡ì„ ì°¾ì„ê¹Œ?"
- Value: "ë¬´ì—‡ì„ ë°˜í™˜í• ê¹Œ?"
- KeyëŠ” ìƒëŒ€ì ìœ¼ë¡œ ëœ ì¤‘ìš”

ì‚¬ìš©:
- ë©”ëª¨ë¦¬ ì œì•½
- ë¹ ë¥¸ ì‹¤í—˜
```

---

## ğŸ› ï¸ 4. ì‹¤ì „ ì„ ì • í”„ë¡œì„¸ìŠ¤

### 4.1 ë‹¨ê³„ë³„ í”„ë¡œì„¸ìŠ¤

#### **Step 1: ëª¨ë¸ êµ¬ì¡° íŒŒì•…**

```python
# 1. ëª¨ë¸ ë¡œë“œ
model = AutoModelForCausalLM.from_pretrained("gpt2")

# 2. Linear ë ˆì´ì–´ ì°¾ê¸°
def print_linear_layers(model, max_display=20):
    count = 0
    for name, module in model.named_modules():
        if isinstance(module, (torch.nn.Linear, torch.nn.Conv1D)):
            if hasattr(module, 'weight'):
                shape = module.weight.shape
                params = shape[0] * shape[1]
                print(f"{name}")
                print(f"  Shape: {shape}")
                print(f"  Params: {params:,}")
                print()
                count += 1
                if count >= max_display:
                    print(f"... (ì´ ë” ë§ì€ ë ˆì´ì–´ ìˆìŒ)")
                    break

print_linear_layers(model)
```

---

#### **Step 2: íŒ¨í„´ ë¶„ì„**

```python
# 3. ëª¨ë“ˆ ì´ë¦„ íŒ¨í„´ ì¶”ì¶œ
def extract_module_patterns(model):
    patterns = set()
    for name, module in model.named_modules():
        if isinstance(module, (torch.nn.Linear, torch.nn.Conv1D)):
            # ë§ˆì§€ë§‰ ë¶€ë¶„ë§Œ ì¶”ì¶œ
            module_name = name.split('.')[-1]
            patterns.add(module_name)
    return sorted(patterns)

patterns = extract_module_patterns(model)
print("ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“ˆ ì´ë¦„:")
for p in patterns:
    print(f"  - {p}")
```

**ì¶œë ¥ (GPT-2)**:
```
ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“ˆ ì´ë¦„:
  - c_attn
  - c_fc
  - c_proj
  - lm_head
```

---

#### **Step 3: Attention ë ˆì´ì–´ ì‹ë³„**

```python
# 4. Attention ê´€ë ¨ ë ˆì´ì–´ ì°¾ê¸°
def find_attention_modules(model):
    attn_modules = set()
    for name, module in model.named_modules():
        if 'attn' in name.lower():
            if isinstance(module, (torch.nn.Linear, torch.nn.Conv1D)):
                module_name = name.split('.')[-1]
                attn_modules.add(module_name)
    return sorted(attn_modules)

attn_modules = find_attention_modules(model)
print("Attention ëª¨ë“ˆ:")
for m in attn_modules:
    print(f"  - {m}")
```

**ì¶œë ¥ (GPT-2)**:
```
Attention ëª¨ë“ˆ:
  - c_attn
  - c_proj
```

---

#### **Step 4: ì´ˆê¸° ì„¤ì • (Attentionë§Œ)**

```python
# 5. ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì‹œì‘
target_modules = ["c_attn", "c_proj"]  # GPT-2

# ë˜ëŠ”
# target_modules = ["q_proj", "k_proj", "v_proj", "o_proj"]  # LLaMA
```

---

#### **Step 5: í…ŒìŠ¤íŠ¸ í•™ìŠµ**

```python
# 6. ì§§ì€ í•™ìŠµìœ¼ë¡œ í…ŒìŠ¤íŠ¸
from peft import LoraConfig, get_peft_model

lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=target_modules,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, lora_config)

# íŒŒë¼ë¯¸í„° í™•ì¸
model.print_trainable_parameters()
```

**ì¶œë ¥**:
```
trainable params: 811,008 || all params: 125,250,816 || trainable%: 0.6475
```

---

#### **Step 6: ì„±ëŠ¥ í‰ê°€**

```python
# 7. ì§§ì€ í•™ìŠµ (1-2 epochs)
# ì„±ëŠ¥ ì¸¡ì •

if performance < target:
    # MLP ì¶”ê°€ ê³ ë ¤
    target_modules = ["c_attn", "c_proj", "c_fc"]
    # ì¬í•™ìŠµ ë° ë¹„êµ
```

---

### 4.2 ê²€ì¦ ë°©ë²•

```python
def verify_target_modules(model, target_modules):
    """Target modulesê°€ ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸"""
    
    # ëª¨ë“  ëª¨ë“ˆ ì´ë¦„ ìˆ˜ì§‘
    all_modules = set()
    for name, module in model.named_modules():
        if isinstance(module, (torch.nn.Linear, torch.nn.Conv1D)):
            module_name = name.split('.')[-1]
            all_modules.add(module_name)
    
    # ê²€ì¦
    print("ê²€ì¦ ê²°ê³¼:")
    for target in target_modules:
        if target in all_modules:
            print(f"  âœ… {target}: ì¡´ì¬í•¨")
        else:
            print(f"  âŒ {target}: ì¡´ì¬í•˜ì§€ ì•ŠìŒ!")
            print(f"     ì‚¬ìš© ê°€ëŠ¥: {all_modules}")
    
    # ì ìš©ë  ë ˆì´ì–´ ìˆ˜ ê³„ì‚°
    count = 0
    for name, module in model.named_modules():
        if isinstance(module, (torch.nn.Linear, torch.nn.Conv1D)):
            module_name = name.split('.')[-1]
            if module_name in target_modules:
                count += 1
    
    print(f"\nì´ {count}ê°œ ë ˆì´ì–´ì— LoRA ì ìš©ë¨")

# ì‚¬ìš©
verify_target_modules(model, ["c_attn", "c_proj"])
```

---

## ğŸ“š 5. ëª¨ë¸ë³„ ê¶Œì¥ ì„¤ì •

### 5.1 GPT-2 (ìš°ë¦¬ í”„ë¡œì íŠ¸)

```python
# ê¸°ë³¸ (ê¶Œì¥)
target_modules = ["c_attn", "c_proj"]

# ì„¤ëª…
c_attn:  QKV projection (768 â†’ 2304)
c_proj:  Output projection (768 â†’ 768)

# íŒŒë¼ë¯¸í„°
r=8 ê¸°ì¤€:
- c_attn: 24,576 params Ã— 12 layers = 294,912
- c_proj: 12,288 params Ã— 12 layers = 147,456
- í•©ê³„: 442,368 params

# í™•ì¥ (ë†’ì€ ì„±ëŠ¥)
target_modules = ["c_attn", "c_proj", "c_fc"]

c_fc: MLP up projection (768 â†’ 3072)
ì¶”ê°€ íŒŒë¼ë¯¸í„°: ~300K
```

---

### 5.2 LLaMA / Mistral

```python
# ê¸°ë³¸ (ê¶Œì¥)
target_modules = [
    "q_proj",   # Query
    "k_proj",   # Key
    "v_proj",   # Value
    "o_proj"    # Output
]

# íš¨ìœ¨ì 
target_modules = ["q_proj", "v_proj"]

# í™•ì¥ (ë†’ì€ ì„±ëŠ¥)
target_modules = [
    "q_proj", "k_proj", "v_proj", "o_proj",
    "gate_proj", "up_proj", "down_proj"
]
```

---

### 5.3 BERT

```python
# ê¸°ë³¸ (ê¶Œì¥)
target_modules = ["query", "key", "value"]

# í™•ì¥
target_modules = ["query", "key", "value", "dense"]

# ì£¼ì˜: "dense"ëŠ” ì—¬ëŸ¬ ê³³ì— ìˆìŒ
# ë” ì •í™•í•œ ì§€ì •:
target_modules = [
    "attention.self.query",
    "attention.self.key",
    "attention.self.value"
]
```

---

## ğŸ¯ ìš°ë¦¬ í”„ë¡œì íŠ¸ ì„ ì • ê³¼ì •

### ì‹¤ì œ ìˆ˜í–‰í•œ ë‹¨ê³„

```python
# Step 1: ëª¨ë¸ í™•ì¸
model_name = "gpt2"
model = AutoModelForCausalLM.from_pretrained(model_name)

# Step 2: êµ¬ì¡° íŒŒì•…
# GPT-2ëŠ” c_attn, c_proj ì‚¬ìš© í™•ì¸

# Step 3: ê¸°ë³¸ ì„¤ì • ì„ íƒ
target_modules = ["c_attn", "c_proj"]

# ì´ìœ :
# âœ… Attention ë ˆì´ì–´ë§Œ (íš¨ìœ¨ì )
# âœ… ê°„ë‹¨í•œ QA íƒœìŠ¤í¬
# âœ… 1,000 ìƒ˜í”Œ (ì‘ì€ ë°ì´í„°)
# âœ… 8GB VRAM (ì¶©ë¶„)

# Step 4: ì ìš©
lora_config = {
    "r": 8,
    "lora_alpha": 16,
    "target_modules": ["c_attn", "c_proj"]
}

# Step 5: ê²°ê³¼
# íŒŒë¼ë¯¸í„°: 811,008 (0.65%)
# Perplexity: 9.08 â†’ 1.05
# ì„±ê³µ! âœ…
```

---

## ğŸ’¡ í•µì‹¬ ìš”ì•½

### **ì„ ì • í”„ë¡œì„¸ìŠ¤**

```
1. ëª¨ë¸ êµ¬ì¡° íŒŒì•…
   â†’ Linear ë ˆì´ì–´ ì°¾ê¸°

2. ëª¨ë“ˆ ì´ë¦„ íŒ¨í„´ ë¶„ì„
   â†’ ì‚¬ìš© ê°€ëŠ¥í•œ ì´ë¦„ í™•ì¸

3. Attention ë ˆì´ì–´ ì‹ë³„
   â†’ ê¸°ë³¸ target ì„ ì •

4. ì´ˆê¸° ì„¤ì • (Attentionë§Œ)
   â†’ í…ŒìŠ¤íŠ¸ í•™ìŠµ

5. ì„±ëŠ¥ í‰ê°€
   â†’ í•„ìš”ì‹œ MLP ì¶”ê°€

6. ìµœì¢… ì„ íƒ
   â†’ ì„±ëŠ¥/ë¹„ìš© ê· í˜•
```

---

### **ê¶Œì¥ ì„¤ì •**

```
ê¸°ë³¸ (ëŒ€ë¶€ë¶„ì˜ ê²½ìš°):
â†’ Attention ë ˆì´ì–´ë§Œ
â†’ GPT-2: ["c_attn", "c_proj"]
â†’ LLaMA: ["q_proj", "k_proj", "v_proj", "o_proj"]

í™•ì¥ (ë†’ì€ ì„±ëŠ¥ í•„ìš”):
â†’ Attention + MLP
â†’ íŒŒë¼ë¯¸í„° 2-3ë°° ì¦ê°€

íš¨ìœ¨ì  (ë©”ëª¨ë¦¬ ì œì•½):
â†’ Query + Valueë§Œ
â†’ íŒŒë¼ë¯¸í„° 50% ì ˆì•½
```

---

ì´ì œ **Target Module ì„ ì • ê³¼ì •**ì„ ì™„ì „íˆ ì´í•´í•˜ì…¨ì„ ê²ƒì…ë‹ˆë‹¤! ğŸš€
