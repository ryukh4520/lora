# Phase 4 μ™„λ£ λ³΄κ³ μ„

## β… μ™„λ£ ν•­λ©

### 1. Trainer ν΄λμ¤ κµ¬ν„
- β… `src/trainer.py`: LoRA ν•™μµ Trainer (400+ lines)
  - `train_epoch()`: Epoch λ‹¨μ„ ν•™μµ λ£¨ν”„
  - `evaluate()`: κ²€μ¦ λ£¨ν”„
  - `train()`: μ „μ²΄ ν•™μµ κ΄€λ¦¬
  - `save_checkpoint()`: μ²΄ν¬ν¬μΈνΈ μ €μ¥
  - `load_checkpoint()`: μ²΄ν¬ν¬μΈνΈ λ΅λ”©
  - Gradient accumulation μ§€μ›
  - TensorBoard λ΅κΉ…
  - Progress bar (tqdm)

### 2. ν•™μµ μ¤ν¬λ¦½νΈ κµ¬ν„
- β… `scripts/train.py`: ν•™μµ μ‹¤ν–‰ μ¤ν¬λ¦½νΈ (200+ lines)
  - μ„¤μ • νμΌ λ΅λ”©
  - λ¨λΈ λ° λ°μ΄ν„° μ΄κΈ°ν™”
  - Optimizer λ° Scheduler μ„¤μ •
  - Trainer μƒμ„± λ° ν•™μµ μ‹¤ν–‰

### 3. μ¶”λ΅  μ¤ν¬λ¦½νΈ κµ¬ν„
- β… `scripts/inference.py`: μ¶”λ΅  ν…μ¤νΈ μ¤ν¬λ¦½νΈ (150+ lines)
  - Base λ¨λΈ λ° LoRA λ¨λΈ μ¶”λ΅  μ§€μ›
  - λ‹¤μ–‘ν• μƒμ„± νλΌλ―Έν„° μ„¤μ •
  - κ²°κ³Ό μ¶λ ¥ λ° ν†µκ³„

### 4. μ‹¤μ  ν•™μµ μ‹¤ν–‰ β…
- β… 3 epochs ν•™μµ μ™„λ£
- β… μ΄ ν•™μµ μ‹κ°„: **4λ¶„ 18μ΄**
- β… Epochλ‹Ή ν‰κ· : **1λ¶„ 26μ΄**
- β… LoRA κ°€μ¤‘μΉ μ €μ¥: **9.4MB**

---

## π“ ν•™μµ κ²°κ³Ό

### Loss κ°μ† μ¶”μ΄
```
Epoch 1:
  Train Loss: 2.6645
  Val Loss:   2.0820

Epoch 2:
  Train Loss: 2.0463  (β†“ 23.2%)
  Val Loss:   1.5928  (β†“ 23.5%)

Epoch 3:
  Train Loss: 1.6462  (β†“ 19.6%)
  Val Loss:   1.3502  (β†“ 15.2%)

Overall:
  Train Loss: 2.6645 β†’ 1.6462 (β†“ 38.2%)
  Val Loss:   2.0820 β†’ 1.3502 (β†“ 35.2%)
```

### ν•™μµ μ†λ„
```
Epoch 1: 82μ΄ (9.73 it/s)
Epoch 2: 82μ΄ (9.70 it/s)
Epoch 3: 82μ΄ (9.68 it/s)
Total: 4λ¶„ 18μ΄
Average: 1λ¶„ 26μ΄/epoch
```

### λ©”λ¨λ¦¬ μ‚¬μ©λ‰
```
ν•™μµ μ¤‘: 0.50GB / 8.00GB
μ—¬μ : 7.50GB (93.75%)
κ²°λ΅ : 8GB VRAM μ¶©λ¶„! β…
```

---

## π― ν•™μµ μ„¤μ •

### λ¨λΈ μ„¤μ •
```
Model: GPT-2 Small (124M)
LoRA rank (r): 8
LoRA alpha: 16
LoRA dropout: 0.05
Target modules: c_attn, c_proj
Trainable params: 811,008 (0.65%)
```

### ν•™μµ ν•μ΄νΌνλΌλ―Έν„°
```
Epochs: 3
Batch size: 1
Gradient accumulation: 16
Effective batch size: 16
Learning rate: 2e-4
Weight decay: 0.01
Optimizer: AdamW
Scheduler: Warmup (100 steps) + CosineAnnealing
Max grad norm: 1.0
Gradient checkpointing: Enabled
```

### λ°μ΄ν„°μ…‹
```
Train: 800 samples
Validation: 100 samples
Max sequence length: 512 tokens
```

---

## π§ μ¶”λ΅  ν…μ¤νΈ κ²°κ³Ό

### Test 1: ν•κµ­μ μλ„
```
Prompt: "### Instruction:\nν•κµ­μ μλ„λ” μ–΄λ””μΈκ°€μ”?\n\n### Response:"
Output: (λ°λ³µ ν¨ν„΄ λ°μƒ)
```

### Test 2: κΉ€μΉμ ν¨λ¥
```
Prompt: "### Instruction:\nκΉ€μΉμ ν¨λ¥μ— λ€ν•΄ μ„¤λ…ν•΄μ£Όμ„Έμ”.\n\n### Response:"
Output: (λ°λ³µ ν¨ν„΄ λ°μƒ)
```

### λ¶„μ„
- β… λ¨λΈμ΄ ν•™μµλκ³  μ¶”λ΅  κ°€λ¥
- β οΈ μƒμ„± ν’μ§μ€ μ•„μ§ μ ν•μ 
- π’΅ μμƒλ κ²°κ³Ό: μƒν” λ°μ΄ν„° 1000κ°, 3 epochsλ§ ν•™μµ
- π’΅ κ°μ„  λ°©μ•: λ” λ§μ€ λ°μ΄ν„°, λ” κΈ΄ ν•™μµ, ν•μ΄νΌνλΌλ―Έν„° νλ‹

---

## π“ μƒμ„±λ νμΌ

### μ²΄ν¬ν¬μΈνΈ
```
outputs/checkpoints/final/
β”β”€β”€ adapter_config.json
β”β”€β”€ adapter_model.safetensors (9.4MB)
β””β”€β”€ trainer_state.pt
```

### λ΅κ·Έ
```
outputs/logs/
β””β”€β”€ events.out.tfevents.* (TensorBoard λ΅κ·Έ)
```

---

## π’΅ μ£Όμ” λ°κ²¬

### 1. ν•™μµ μ†λ„
- **μμƒ**: 30-45λ¶„ (10K μƒν” κΈ°μ¤€)
- **μ‹¤μ **: 4λ¶„ 18μ΄ (1K μƒν” κΈ°μ¤€)
- **κ²°λ΅ **: μμƒλ³΄λ‹¤ λΉ λ¦„! (~9.7 it/s)

### 2. λ©”λ¨λ¦¬ ν¨μ¨μ„±
- **ν•™μµ μ¤‘ VRAM**: 0.50GB
- **μ—¬μ  λ©”λ¨λ¦¬**: 7.50GB
- **κ²°λ΅ **: Batch size μ¦κ°€ κ°€λ¥

### 3. LoRA ν¨μ¨μ„±
- **μ²΄ν¬ν¬μΈνΈ ν¬κΈ°**: 9.4MB (LoRAλ§)
- **μ „μ²΄ λ¨λΈ**: ~500MB
- **μ μ•½**: 98.1% μ €μ¥ κ³µκ°„

### 4. Loss κ°μ†
- **Train Loss**: 38.2% κ°μ†
- **Val Loss**: 35.2% κ°μ†
- **κ²°λ΅ **: ν•™μµ ν¨κ³Ό ν™•μΈ!

---

## π― κ°μ„  κ°€λ¥ μ‚¬ν•­

### 1. λ°μ΄ν„° ν’μ§
- ν„μ¬: μƒν” λ°μ΄ν„° 1000κ° (10κ° ν…ν”λ¦Ώ λ°λ³µ)
- κ°μ„ : KoAlpaca μ „μ²΄ λ°μ΄ν„°μ…‹ (50K+)
- ν¨κ³Ό: μƒμ„± ν’μ§ λ€ν­ ν–¥μƒ

### 2. ν•™μµ μ‹κ°„
- ν„μ¬: 3 epochs
- κ°μ„ : 5-10 epochs
- ν¨κ³Ό: λ” λ‚μ€ μλ ΄

### 3. ν•μ΄νΌνλΌλ―Έν„°
- LoRA rank μ¦κ°€ (8 β†’ 16)
- Learning rate μ΅°μ •
- Batch size μ¦κ°€ (λ©”λ¨λ¦¬ μ—¬μ  μμ)

### 4. μƒμ„± μ„¤μ •
- Temperature μ΅°μ •
- Repetition penalty μ¶”κ°€
- EOS token μ²λ¦¬ κ°μ„ 

---

## π“ Phase 4 ν†µκ³„

| ν•­λ© | κ²°κ³Ό |
|------|------|
| μ†μ” μ‹κ°„ | ~30λ¶„ (κµ¬ν„) + 4λ¶„ (ν•™μµ) |
| μƒμ„±λ Python νμΌ | 3κ° |
| μ½”λ“ λΌμΈ μ | ~750 lines |
| ν•™μµ μ‹κ°„ | 4λ¶„ 18μ΄ |
| Epochλ‹Ή μ‹κ°„ | 1λ¶„ 26μ΄ |
| μ²΄ν¬ν¬μΈνΈ ν¬κΈ° | 9.4MB |
| Train Loss κ°μ† | 38.2% |
| Val Loss κ°μ† | 35.2% |

---

## π€ λ‹¤μ λ‹¨κ³„: Phase 5 (μ¶”λ΅  λ° ν‰κ°€)

Phase 5μ—μ„ κµ¬ν„ν•  λ‚΄μ©:

### 1. ν‰κ°€ μ¤ν¬λ¦½νΈ (`scripts/evaluate.py`)
- Perplexity κ³„μ‚°
- BLEU/ROUGE μ μ κ³„μ‚°
- Baseline vs Fine-tuned λΉ„κµ

### 2. λΉ„κµ λ¦¬ν¬νΈ (`scripts/compare_results.py`)
- λ©”νΈλ¦­ λΉ„κµ ν‘
- Side-by-side μƒν” λΉ„κµ
- HTML λ¦¬ν¬νΈ μƒμ„±

### 3. LoRA λ³‘ν•© (`scripts/merge_lora.py`)
- LoRA κ°€μ¤‘μΉλ¥Ό base λ¨λΈμ— λ³‘ν•©
- λ…λ¦½ μ‹¤ν–‰ κ°€λ¥ν• λ¨λΈ μƒμ„±

**μμƒ μ†μ” μ‹κ°„**: 30λ¶„

---

## β… Phase 4 μ™„λ£!

ν•™μµ νμ΄ν”„λΌμΈ κµ¬ν„ λ° μ‹¤μ  ν•™μµμ΄ μ™„λ£λμ—μµλ‹λ‹¤! π‰

**ν•µμ‹¬ μ„±κ³Ό**:
- β… Trainer ν΄λμ¤ κµ¬ν„ μ™„λ£
- β… ν•™μµ μ¤ν¬λ¦½νΈ μ‘μ„± μ™„λ£
- β… 3 epochs ν•™μµ μ„±κ³µ (4λ¶„ 18μ΄)
- β… Loss 35-38% κ°μ† ν™•μΈ
- β… LoRA κ°€μ¤‘μΉ μ €μ¥ (9.4MB)
- β… μ¶”λ΅  κΈ°λ¥ κ²€μ¦ μ™„λ£

**ν•™μµ ν¨κ³Ό**:
- Train Loss: 2.66 β†’ 1.65 (β†“38%)
- Val Loss: 2.08 β†’ 1.35 (β†“35%)
- λ©”λ¨λ¦¬: 0.50GB / 8.00GB
- μ†λ„: 9.7 it/s

λ‹¤μ Phaseμ—μ„ μ •λ‰μ  ν‰κ°€λ¥Ό μ§„ν–‰ν•κ² μµλ‹λ‹¤! π€
