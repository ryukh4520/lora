# LoRA í•™ìŠµ ë©”ì»¤ë‹ˆì¦˜ ì •í™•í•œ ì´í•´

## ğŸ¯ í•µì‹¬ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€

**ì§ˆë¬¸**: "ì–´í…ì…˜ ëª¨ë“ˆ + MLPì— ëŒ€í•´ì„œ dimensionì´ 8ì¸ í–‰ë ¬ì„ ìƒì„±í•´ì„œ í•´ë‹¹ í–‰ë ¬ë§Œ ì¶”ê°€ í•™ìŠµí•˜ëŠ”ê±´ê°€?"

**ë‹µë³€**: ê±°ì˜ ë§ì§€ë§Œ, ì •í™•íˆëŠ” **dimensionì´ 8ì¸ í–‰ë ¬ 2ê°œ (Aì™€ B)**ë¥¼ ìƒì„±í•©ë‹ˆë‹¤!

---

## ğŸ“ ì •í™•í•œ ë©”ì»¤ë‹ˆì¦˜

### ìš°ë¦¬ í”„ë¡œì íŠ¸ ì„¤ì •

```python
# ì„¤ì •
r = 8
target_modules = ["c_attn", "c_proj"]  # Attentionë§Œ (MLP ì œì™¸)
```

---

### ê° ë ˆì´ì–´ë§ˆë‹¤ ìƒì„±ë˜ëŠ” ê²ƒ

#### **1. c_attn ë ˆì´ì–´ (768 â†’ 2304)**

```python
# ì›ë˜ ê°€ì¤‘ì¹˜ (Frozen â„ï¸)
W_attn: (2304, 768)  # 1,769,472 params
â†’ í•™ìŠµ ì•ˆí•¨! (requires_grad=False)

# LoRA í–‰ë ¬ A (Trainable ğŸ”¥)
A_attn: (8, 768)  # 6,144 params
â†’ í•™ìŠµí•¨! (requires_grad=True)

# LoRA í–‰ë ¬ B (Trainable ğŸ”¥)
B_attn: (2304, 8)  # 18,432 params
â†’ í•™ìŠµí•¨! (requires_grad=True)

# ì´ LoRA íŒŒë¼ë¯¸í„°
6,144 + 18,432 = 24,576 params
```

**ì‹œê°í™”**:
```
ì›ë˜:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   W (2304Ã—768)  â”‚  1,769,472 params â„ï¸
â”‚   (Frozen)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

LoRA ì¶”ê°€:
â”Œâ”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ B â”‚ Ã— â”‚    A    â”‚  24,576 params ğŸ”¥
â”‚   â”‚ 8 â”‚         â”‚  (í•™ìŠµë¨!)
â””â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
2304Ã—8    8Ã—768
```

---

#### **2. c_proj ë ˆì´ì–´ (768 â†’ 768)**

```python
# ì›ë˜ ê°€ì¤‘ì¹˜ (Frozen â„ï¸)
W_proj: (768, 768)  # 589,824 params
â†’ í•™ìŠµ ì•ˆí•¨!

# LoRA í–‰ë ¬ A (Trainable ğŸ”¥)
A_proj: (8, 768)  # 6,144 params
â†’ í•™ìŠµí•¨!

# LoRA í–‰ë ¬ B (Trainable ğŸ”¥)
B_proj: (768, 8)  # 6,144 params
â†’ í•™ìŠµí•¨!

# ì´ LoRA íŒŒë¼ë¯¸í„°
6,144 + 6,144 = 12,288 params
```

---

### ì „ì²´ ëª¨ë¸ (12 layers)

```python
# GPT-2ëŠ” 12ê°œ ë ˆì´ì–´
# ê° ë ˆì´ì–´ë§ˆë‹¤ c_attn, c_proj ìˆìŒ

Layer 0:
  c_attn:  A (8Ã—768) + B (2304Ã—8) = 24,576 params
  c_proj:  A (8Ã—768) + B (768Ã—8)  = 12,288 params

Layer 1:
  c_attn:  A (8Ã—768) + B (2304Ã—8) = 24,576 params
  c_proj:  A (8Ã—768) + B (768Ã—8)  = 12,288 params

...

Layer 11:
  c_attn:  A (8Ã—768) + B (2304Ã—8) = 24,576 params
  c_proj:  A (8Ã—768) + B (768Ã—8)  = 12,288 params

# ì´ LoRA íŒŒë¼ë¯¸í„°
12 layers Ã— (24,576 + 12,288) = 12 Ã— 36,864 = 442,368 params

# ì‹¤ì œ ì¸¡ì • (Dropout ë“± í¬í•¨)
811,008 params
```

---

## ğŸ” ì •í™•í•œ ì´í•´

### **1ê°œ í–‰ë ¬ì´ ì•„ë‹ˆë¼ 2ê°œ í–‰ë ¬**

```python
# âŒ ì˜ëª»ëœ ì´í•´
"dimension 8ì¸ í–‰ë ¬ 1ê°œ"

# âœ… ì •í™•í•œ ì´í•´
"dimension 8ì„ í¬í•¨í•˜ëŠ” í–‰ë ¬ 2ê°œ (Aì™€ B)"

A: (r, in_features)   # r=8
B: (out_features, r)  # r=8

Î”W = B @ A  # Low-rank decomposition
```

---

### **ì™œ 2ê°œì¸ê°€?**

```python
# ëª©í‘œ: Î”W âˆˆ â„^(d_out Ã— d_in) ê·¼ì‚¬

# ì§ì ‘ í•™ìŠµ (Full Fine-tuning)
Î”W: (d_out, d_in)  # ëª¨ë“  ì›ì†Œ í•™ìŠµ
íŒŒë¼ë¯¸í„°: d_out Ã— d_in

# LoRA (Low-rank)
A: (r, d_in)       # rê°œ í–‰ í•™ìŠµ
B: (d_out, r)      # rê°œ ì—´ í•™ìŠµ
Î”W = B @ A         # í–‰ë ¬ ê³±ìœ¼ë¡œ ì¬êµ¬ì„±
íŒŒë¼ë¯¸í„°: r Ã— (d_in + d_out)

# ì˜ˆì‹œ (768 â†’ 2304, r=8)
Full: 768 Ã— 2304 = 1,769,472 params
LoRA: 8 Ã— (768 + 2304) = 24,576 params
ì ˆì•½: 98.6%!
```

---

## ğŸ’» ì‹¤ì œ ì½”ë“œë¡œ ì´í•´

### Forward Pass

```python
# ì…ë ¥
x: (batch, 768)

# === c_attn ë ˆì´ì–´ ===

# 1. ì›ë˜ ê°€ì¤‘ì¹˜ (Frozen)
W: (2304, 768)
h_base = x @ W.T  # (batch, 2304)

# 2. LoRA A (Trainable)
A: (8, 768)
temp = x @ A.T  # (batch, 8) â† 768ì°¨ì› â†’ 8ì°¨ì› ì••ì¶•!

# 3. LoRA B (Trainable)
B: (2304, 8)
h_lora = temp @ B.T  # (batch, 2304) â† 8ì°¨ì› â†’ 2304ì°¨ì› í™•ì¥!

# 4. Scaling
h_lora = h_lora * (alpha/r)  # (batch, 2304)

# 5. ê²°í•©
h = h_base + h_lora  # (batch, 2304)
```

**í•µì‹¬**:
```
x (768ì°¨ì›)
  â†“
A @ x (8ì°¨ì›) â† ë³‘ëª©(bottleneck)!
  â†“
B @ (A @ x) (2304ì°¨ì›)
  â†“
ìµœì¢… ì¶œë ¥
```

---

## ğŸ¯ ìš°ë¦¬ í”„ë¡œì íŠ¸ ì •í™•í•œ ì„¤ëª…

### **ì„¤ì •**

```python
r = 8
target_modules = ["c_attn", "c_proj"]  # Attentionë§Œ
# MLPëŠ” í¬í•¨ ì•ˆí•¨! (c_fc ì œì™¸)
```

---

### **ìƒì„±ë˜ëŠ” í–‰ë ¬**

```python
# ê° c_attn ë ˆì´ì–´ë§ˆë‹¤ (12ê°œ):
A_attn: (8, 768)    # 6,144 params
B_attn: (2304, 8)   # 18,432 params

# ê° c_proj ë ˆì´ì–´ë§ˆë‹¤ (12ê°œ):
A_proj: (8, 768)    # 6,144 params
B_proj: (768, 8)    # 6,144 params

# ì´ 24ê°œ ë ˆì´ì–´ (12 layers Ã— 2 modules)
# ì´ 48ê°œ í–‰ë ¬ (24 layers Ã— 2 matrices)
```

---

### **í•™ìŠµë˜ëŠ” ê²ƒ**

```python
# âœ… í•™ìŠµë¨ (Trainable)
- ëª¨ë“  A í–‰ë ¬ (24ê°œ)
- ëª¨ë“  B í–‰ë ¬ (24ê°œ)
- ì´ 48ê°œ í–‰ë ¬, 811,008 params

# âŒ í•™ìŠµ ì•ˆë¨ (Frozen)
- ëª¨ë“  W í–‰ë ¬ (ì›ë˜ ê°€ì¤‘ì¹˜)
- 124,439,808 params
```

---

## ğŸ“Š ì‹œê°í™”

### **ì „ì²´ êµ¬ì¡°**

```
GPT-2 (12 layers)

Layer 0:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Attention                   â”‚
  â”‚                             â”‚
  â”‚ c_attn:                     â”‚
  â”‚   W (Frozen) â„ï¸             â”‚
  â”‚   + A (8Ã—768) ğŸ”¥            â”‚
  â”‚   + B (2304Ã—8) ğŸ”¥           â”‚
  â”‚                             â”‚
  â”‚ c_proj:                     â”‚
  â”‚   W (Frozen) â„ï¸             â”‚
  â”‚   + A (8Ã—768) ğŸ”¥            â”‚
  â”‚   + B (768Ã—8) ğŸ”¥            â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ MLP                         â”‚
  â”‚                             â”‚
  â”‚ c_fc:                       â”‚
  â”‚   W (Frozen) â„ï¸             â”‚
  â”‚   (LoRA ì—†ìŒ)               â”‚
  â”‚                             â”‚
  â”‚ c_proj:                     â”‚
  â”‚   W (Frozen) â„ï¸             â”‚
  â”‚   (LoRA ì—†ìŒ)               â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Layer 1:
  (ë™ì¼ êµ¬ì¡° ë°˜ë³µ)
  ...

Layer 11:
  (ë™ì¼ êµ¬ì¡° ë°˜ë³µ)
```

---

## ğŸ“ í•µì‹¬ ì •ë¦¬

### **ì •í™•í•œ ì´í•´**

```
1. ê° target moduleë§ˆë‹¤:
   â†’ A í–‰ë ¬ 1ê°œ (r Ã— in_features)
   â†’ B í–‰ë ¬ 1ê°œ (out_features Ã— r)
   â†’ ì´ 2ê°œ í–‰ë ¬ ìƒì„±

2. r=8ì˜ ì˜ë¯¸:
   â†’ Aì™€ Bë¥¼ ì—°ê²°í•˜ëŠ” "ë³‘ëª©" ì°¨ì›
   â†’ 768ì°¨ì› â†’ 8ì°¨ì› â†’ 2304ì°¨ì›

3. í•™ìŠµë˜ëŠ” ê²ƒ:
   â†’ A, B í–‰ë ¬ë§Œ (48ê°œ)
   â†’ ì›ë˜ ê°€ì¤‘ì¹˜ WëŠ” ë™ê²°

4. ìš°ë¦¬ í”„ë¡œì íŠ¸:
   â†’ Attentionë§Œ (c_attn, c_proj)
   â†’ MLP ì œì™¸ (c_fc ì—†ìŒ)
   â†’ ì´ 811,008 params í•™ìŠµ
```

---

### **ë¹„ìœ **

```
ì›ë˜ ëª¨ë¸ = ê±°ëŒ€í•œ ê±´ë¬¼ (124M ë²½ëŒ)
LoRA = ì‘ì€ í™•ì¥ ê³µì‚¬ (0.8M ë²½ëŒ)

ê±´ë¬¼ì€ ê·¸ëŒ€ë¡œ ë‘ê³  (Frozen)
ì‘ì€ í™•ì¥ë§Œ ì¶”ê°€ (A, B í–‰ë ¬)

í™•ì¥ ì„¤ê³„:
- ê° ë°©(ë ˆì´ì–´)ë§ˆë‹¤
- 2ê°œì˜ ì‘ì€ êµ¬ì¡°ë¬¼ (A, B)
- 8ê°œ ê¸°ë‘¥(dimension)ìœ¼ë¡œ ì—°ê²°
```

---

## âœ… ì§ˆë¬¸ì— ëŒ€í•œ ìµœì¢… ë‹µë³€

**ì›ë˜ ì§ˆë¬¸**: "ì–´í…ì…˜ ëª¨ë“ˆ + MLPì— ëŒ€í•´ì„œ dimensionì´ 8ì¸ í–‰ë ¬ì„ ìƒì„±í•´ì„œ í•´ë‹¹ í–‰ë ¬ë§Œ ì¶”ê°€ í•™ìŠµí•˜ëŠ”ê±´ê°€?"

**ì •í™•í•œ ë‹µë³€**:

```
1. Attention ëª¨ë“ˆë§Œ (MLP ì œì™¸)
   â†’ target_modules = ["c_attn", "c_proj"]
   â†’ c_fc (MLP)ëŠ” í¬í•¨ ì•ˆë¨

2. Dimension 8ì¸ í–‰ë ¬ 2ê°œ
   â†’ A: (8, in_features)
   â†’ B: (out_features, 8)

3. ê° ë ˆì´ì–´ë§ˆë‹¤ ìƒì„±
   â†’ 12 layers Ã— 2 modules = 24ê°œ ë ˆì´ì–´
   â†’ 24 Ã— 2 matrices = 48ê°œ í–‰ë ¬

4. í•´ë‹¹ í–‰ë ¬ë§Œ í•™ìŠµ
   â†’ A, Bë§Œ í•™ìŠµ (811,008 params)
   â†’ WëŠ” ë™ê²° (124M params)
```

---

ì´ì œ ì™„ë²½í•˜ê²Œ ì´í•´í•˜ì…¨ì„ ê²ƒì…ë‹ˆë‹¤! ğŸš€
