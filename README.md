# LoRA Fine-tuning Demo Project

GPT-2 Small ëª¨ë¸ì„ LoRA(Low-Rank Adaptation)ë¥¼ í†µí•´ íš¨ìœ¨ì ìœ¼ë¡œ íŒŒì¸íŠœë‹í•˜ëŠ” ë°ëª¨ í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤.

## ğŸ¯ í”„ë¡œì íŠ¸ ê°œìš”

- **ëª¨ë¸**: GPT-2 Small (124M parameters)
- **ë°©ë²•**: LoRA (Parameter-Efficient Fine-Tuning)
- **í™˜ê²½**: RTX 3070 (8GB VRAM)
- **ì˜ˆìƒ í•™ìŠµ ì‹œê°„**: 30-45ë¶„ (10K ìƒ˜í”Œ, 3 epochs)

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
lora/
â”œâ”€â”€ README.md                    # í”„ë¡œì íŠ¸ ì„¤ëª…ì„œ
â”œâ”€â”€ PROJECT_PLAN.md             # ìƒì„¸ ì„¤ê³„ ë¬¸ì„œ
â”œâ”€â”€ EVALUATION_STRATEGY.md      # í‰ê°€ ì „ëµ
â”œâ”€â”€ requirements.txt            # Python ì˜ì¡´ì„±
â”œâ”€â”€ Dockerfile                  # Docker ì´ë¯¸ì§€ ì •ì˜
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ model_config.yaml      # ëª¨ë¸ ì„¤ì •
â”‚   â””â”€â”€ training_config.yaml   # í•™ìŠµ ì„¤ì •
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                   # ì›ë³¸ ë°ì´í„°
â”‚   â”œâ”€â”€ processed/             # ì „ì²˜ë¦¬ëœ ë°ì´í„°
â”‚   â””â”€â”€ prepare_dataset.py     # ë°ì´í„° ì „ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ model.py               # ëª¨ë¸ ë¡œë”© ë° LoRA ì„¤ì •
â”‚   â”œâ”€â”€ dataset.py             # ë°ì´í„°ì…‹ í´ë˜ìŠ¤
â”‚   â”œâ”€â”€ trainer.py             # í•™ìŠµ ë¡œì§
â”‚   â””â”€â”€ utils.py               # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train.py               # í•™ìŠµ ì‹¤í–‰
â”‚   â”œâ”€â”€ inference.py           # ì¶”ë¡  í…ŒìŠ¤íŠ¸
â”‚   â”œâ”€â”€ evaluate.py            # í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ compare_results.py     # ê²°ê³¼ ë¹„êµ
â”‚   â””â”€â”€ merge_lora.py          # LoRA ê°€ì¤‘ì¹˜ ë³‘í•©
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ demo.ipynb             # ë°ëª¨ ë…¸íŠ¸ë¶
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ checkpoints/           # í•™ìŠµ ì²´í¬í¬ì¸íŠ¸
â”‚   â”œâ”€â”€ logs/                  # í•™ìŠµ ë¡œê·¸
â”‚   â”œâ”€â”€ eval/                  # í‰ê°€ ê²°ê³¼
â”‚   â””â”€â”€ merged_models/         # ë³‘í•©ëœ ëª¨ë¸
â””â”€â”€ tests/
    â””â”€â”€ test_model.py          # ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
```

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### 1. Docker í™˜ê²½ ì„¤ì •

#### Docker ì´ë¯¸ì§€ ë¹Œë“œ
```bash
docker build -t lora-training .
```

#### Docker ì»¨í…Œì´ë„ˆ ì‹¤í–‰
```bash
docker run -it --gpus all \
    -v $(pwd):/workspace \
    --name lora_demo \
    lora-training
```

### 2. ë¡œì»¬ í™˜ê²½ ì„¤ì • (ëŒ€ì•ˆ)

```bash
# ê°€ìƒí™˜ê²½ ìƒì„±
python3 -m venv venv
source venv/bin/activate  # Linux/Mac
# venv\Scripts\activate   # Windows

# íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install -r requirements.txt
```

### 3. ë°ì´í„° ì¤€ë¹„

```bash
# ìƒ˜í”Œ ë°ì´í„° ë‹¤ìš´ë¡œë“œ ë° ì „ì²˜ë¦¬
python data/prepare_dataset.py --dataset koalpaca --num_samples 10000
```

### 4. í•™ìŠµ ì‹¤í–‰

```bash
# Baseline í‰ê°€ (í•™ìŠµ ì „)
python scripts/evaluate.py --mode baseline

# LoRA í•™ìŠµ
python scripts/train.py

# Fine-tuned í‰ê°€ (í•™ìŠµ í›„)
python scripts/evaluate.py --mode finetuned

# ê²°ê³¼ ë¹„êµ
python scripts/compare_results.py
```

### 5. ì¶”ë¡  í…ŒìŠ¤íŠ¸

```bash
python scripts/inference.py \
    --base_model gpt2 \
    --lora_weights outputs/checkpoints/final \
    --prompt "í•œêµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?"
```

## âš™ï¸ ì„¤ì • ì»¤ìŠ¤í„°ë§ˆì´ì§•

### ëª¨ë¸ ì„¤ì • (`config/model_config.yaml`)
- LoRA rank, alpha, dropout
- Target modules
- Generation parameters

### í•™ìŠµ ì„¤ì • (`config/training_config.yaml`)
- Batch size, learning rate
- Epochs, warmup steps
- Logging, checkpointing

## ğŸ“Š í‰ê°€ ë©”íŠ¸ë¦­

- **Perplexity**: ì–¸ì–´ ëª¨ë¸ ì„±ëŠ¥ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
- **BLEU Score**: ìƒì„± í’ˆì§ˆ (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)
- **ROUGE Score**: ìš”ì•½ í’ˆì§ˆ
- **Human Evaluation**: ì •ì„±ì  í‰ê°€

## ğŸ¯ ì„±ê³µ ê¸°ì¤€

- âœ… Perplexity 10-20% ê°ì†Œ
- âœ… BLEU Score 5-10ì  ì¦ê°€
- âœ… ìƒ˜í”Œ ìƒì„± í’ˆì§ˆ í–¥ìƒ

## ğŸ“ ì£¼ìš” ëª…ë ¹ì–´

```bash
# GPU í™•ì¸
nvidia-smi

# í•™ìŠµ ëª¨ë‹ˆí„°ë§
tensorboard --logdir outputs/logs

# ì²´í¬í¬ì¸íŠ¸ í™•ì¸
ls -lh outputs/checkpoints/

# LoRA ê°€ì¤‘ì¹˜ ë³‘í•©
python scripts/merge_lora.py \
    --base_model gpt2 \
    --lora_weights outputs/checkpoints/final \
    --output_dir outputs/merged_models/gpt2-lora
```

## ğŸ› ë¬¸ì œ í•´ê²°

### OOM (Out of Memory) ì—ëŸ¬
```yaml
# training_config.yaml ìˆ˜ì •
batch_size: 1
gradient_accumulation_steps: 8  # 16ì—ì„œ ê°ì†Œ
max_seq_length: 256  # 512ì—ì„œ ê°ì†Œ
```

### í•™ìŠµ ë¶ˆì•ˆì •
```yaml
# training_config.yaml ìˆ˜ì •
learning_rate: 1.0e-4  # 2.0e-4ì—ì„œ ê°ì†Œ
warmup_steps: 200  # 100ì—ì„œ ì¦ê°€
```

## ğŸ“š ì°¸ê³  ìë£Œ

- [LoRA ë…¼ë¬¸](https://arxiv.org/abs/2106.09685)
- [Hugging Face PEFT](https://github.com/huggingface/peft)
- [GPT-2 ëª¨ë¸](https://huggingface.co/gpt2)

## ğŸ“„ ë¼ì´ì„ ìŠ¤

MIT License

## ğŸ¤ ê¸°ì—¬

ì´ìŠˆ ë° PR í™˜ì˜í•©ë‹ˆë‹¤!

## ğŸ“§ ë¬¸ì˜

í”„ë¡œì íŠ¸ ê´€ë ¨ ë¬¸ì˜ì‚¬í•­ì€ ì´ìŠˆë¡œ ë‚¨ê²¨ì£¼ì„¸ìš”.
