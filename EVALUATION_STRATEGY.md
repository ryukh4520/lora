# LoRA ì„±ëŠ¥ í‰ê°€ ì „ëµ

## ğŸ¯ í‰ê°€ ëª©í‘œ

LoRA íŒŒì¸íŠœë‹ ì „í›„ì˜ ëª¨ë¸ ì„±ëŠ¥ì„ **ì •ëŸ‰ì **ì´ê³  **ì •ì„±ì **ìœ¼ë¡œ ë¹„êµí•˜ì—¬, í•™ìŠµ íš¨ê³¼ë¥¼ ëª…í™•íˆ ê²€ì¦í•©ë‹ˆë‹¤.

---

## ğŸ“Š í‰ê°€ ë°©ë²•ë¡ 

### 1. **Baseline vs Fine-tuned ë¹„êµ**

```
[ì›ë³¸ GPT-2 ëª¨ë¸] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                            â”œâ”€â”€> ë™ì¼ í”„ë¡¬í”„íŠ¸ ì…ë ¥ â”€â”€> ê²°ê³¼ ë¹„êµ
[GPT-2 + LoRA ëª¨ë¸] â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ” í‰ê°€ ë©”íŠ¸ë¦­ (Metrics)

### A. ì •ëŸ‰ì  í‰ê°€ (Quantitative Evaluation)

#### 1ï¸âƒ£ **Perplexity (PPL)** - ì–¸ì–´ ëª¨ë¸ ì„±ëŠ¥ì˜ ê¸°ë³¸ ì§€í‘œ
```python
# ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ (ëª¨ë¸ì´ í…ìŠ¤íŠ¸ë¥¼ ì–¼ë§ˆë‚˜ ì˜ ì˜ˆì¸¡í•˜ëŠ”ì§€)
PPL = exp(average_loss)
```

**ì¸¡ì • ë°©ë²•**:
- ê²€ì¦ ë°ì´í„°ì…‹ì— ëŒ€í•œ Cross-Entropy Loss ê³„ì‚°
- LoRA ì ìš© ì „í›„ ë¹„êµ
- **ëª©í‘œ**: PPL ê°ì†Œ (ì˜ˆ: 50 â†’ 30)

**êµ¬í˜„**:
```python
from torch.nn import CrossEntropyLoss

def calculate_perplexity(model, dataloader):
    model.eval()
    total_loss = 0
    with torch.no_grad():
        for batch in dataloader:
            outputs = model(**batch)
            total_loss += outputs.loss.item()
    
    avg_loss = total_loss / len(dataloader)
    perplexity = torch.exp(torch.tensor(avg_loss))
    return perplexity.item()
```

---

#### 2ï¸âƒ£ **BLEU Score** - ìƒì„± í’ˆì§ˆ í‰ê°€ (ë²ˆì—­/ìš”ì•½ íƒœìŠ¤í¬)
```python
# ìƒì„±ëœ í…ìŠ¤íŠ¸ì™€ ì •ë‹µ í…ìŠ¤íŠ¸ì˜ ìœ ì‚¬ë„ (0~100)
# ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ
```

**ì¸¡ì • ë°©ë²•**:
- í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì˜ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±
- ì •ë‹µê³¼ ë¹„êµí•˜ì—¬ BLEU ì ìˆ˜ ê³„ì‚°
- **ëª©í‘œ**: BLEU ì¦ê°€ (ì˜ˆ: 15 â†’ 35)

**êµ¬í˜„**:
```python
from sacrebleu import corpus_bleu

def evaluate_bleu(model, test_data):
    predictions = []
    references = []
    
    for item in test_data:
        pred = generate_response(model, item['input'])
        predictions.append(pred)
        references.append([item['output']])
    
    bleu = corpus_bleu(predictions, references)
    return bleu.score
```

---

#### 3ï¸âƒ£ **ROUGE Score** - ìš”ì•½ í’ˆì§ˆ í‰ê°€
```python
# ROUGE-1, ROUGE-2, ROUGE-L
# ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ (0~1)
```

**ì¸¡ì • ë°©ë²•**:
- ìƒì„±ëœ í…ìŠ¤íŠ¸ì™€ ì°¸ì¡° í…ìŠ¤íŠ¸ì˜ n-gram ê²¹ì¹¨ ë¹„ìœ¨
- **ëª©í‘œ**: ROUGE-L ì¦ê°€ (ì˜ˆ: 0.25 â†’ 0.45)

---

#### 4ï¸âƒ£ **Accuracy (ì •í™•ë„)** - ë¶„ë¥˜/QA íƒœìŠ¤í¬
```python
# ì •ë‹µë¥  (0~100%)
# ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ
```

**ì¸¡ì • ë°©ë²•**:
- ê°ê´€ì‹ ì§ˆë¬¸ì´ë‚˜ ë¶„ë¥˜ íƒœìŠ¤í¬ì—ì„œ ì •ë‹µ ë¹„ìœ¨
- **ëª©í‘œ**: Accuracy ì¦ê°€ (ì˜ˆ: 45% â†’ 75%)

---

### B. ì •ì„±ì  í‰ê°€ (Qualitative Evaluation)

#### 1ï¸âƒ£ **Human Evaluation (ì¸ê°„ í‰ê°€)**

**í‰ê°€ ê¸°ì¤€**:
1. **ìœ ì°½ì„± (Fluency)**: ë¬¸ë²•ì ìœ¼ë¡œ ìì—°ìŠ¤ëŸ¬ìš´ê°€? (1-5ì )
2. **ê´€ë ¨ì„± (Relevance)**: ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë‹µë³€ì¸ê°€? (1-5ì )
3. **ì •í™•ì„± (Correctness)**: ì‚¬ì‹¤ì ìœ¼ë¡œ ì •í™•í•œê°€? (1-5ì )
4. **ìœ ìš©ì„± (Helpfulness)**: ì‹¤ì œë¡œ ë„ì›€ì´ ë˜ëŠ”ê°€? (1-5ì )

**í‰ê°€ í”„ë¡œì„¸ìŠ¤**:
```
1. 20-50ê°œ í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ì¤€ë¹„
2. Baselineê³¼ Fine-tuned ëª¨ë¸ë¡œ ê°ê° ìƒì„±
3. í‰ê°€ìê°€ ë¸”ë¼ì¸ë“œ í…ŒìŠ¤íŠ¸ (ì–´ëŠ ê²ƒì´ LoRAì¸ì§€ ëª¨ë¥´ê²Œ)
4. ì ìˆ˜ ì§‘ê³„ ë° í†µê³„ ë¶„ì„
```

---

#### 2ï¸âƒ£ **Side-by-Side Comparison (ë³‘ë ¬ ë¹„êµ)**

**ì˜ˆì‹œ í…œí”Œë¦¿**:
```
í”„ë¡¬í”„íŠ¸: "í•œêµ­ì˜ ì „í†µ ìŒì‹ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”."

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Baseline (ì›ë³¸ GPT-2)                                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ í•œêµ­ì˜ ì „í†µ ìŒì‹ì€ ê¹€ì¹˜, ë¶ˆê³ ê¸° ë“±ì´ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ      â”‚
â”‚ ìŒì‹ë“¤ì€ ë§›ìˆê³  ê±´ê°•ì— ì¢‹ìŠµë‹ˆë‹¤...                          â”‚
â”‚                                                             â”‚
â”‚ [í‰ê°€] ìœ ì°½ì„±: 3/5, ê´€ë ¨ì„±: 4/5, ì •í™•ì„±: 3/5               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Fine-tuned (GPT-2 + LoRA)                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ í•œêµ­ì˜ ëŒ€í‘œì ì¸ ì „í†µ ìŒì‹ìœ¼ë¡œëŠ” ê¹€ì¹˜, ë¶ˆê³ ê¸°, ë¹„ë¹”ë°¥ ë“±ì´  â”‚
â”‚ ìˆìŠµë‹ˆë‹¤. ê¹€ì¹˜ëŠ” ë°°ì¶”ë¥¼ ë°œíš¨ì‹œí‚¨ ìŒì‹ìœ¼ë¡œ ìœ ì‚°ê· ì´ í’ë¶€í•˜ë©°â”‚
â”‚ í•œêµ­ì¸ì˜ ì‹íƒì—ì„œ ë¹ ì§ˆ ìˆ˜ ì—†ëŠ” ë°˜ì°¬ì…ë‹ˆë‹¤...                â”‚
â”‚                                                             â”‚
â”‚ [í‰ê°€] ìœ ì°½ì„±: 5/5, ê´€ë ¨ì„±: 5/5, ì •í™•ì„±: 5/5               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ§ª í‰ê°€ ë°ì´í„°ì…‹ êµ¬ì„±

### 1. **í•™ìŠµ ë°ì´í„°ì™€ ë¶„ë¦¬ëœ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸**
```
ì „ì²´ ë°ì´í„° (12,000 ìƒ˜í”Œ)
â”œâ”€â”€ í•™ìŠµ (Train): 10,000 ìƒ˜í”Œ (83%)
â”œâ”€â”€ ê²€ì¦ (Validation): 1,000 ìƒ˜í”Œ (8%)
â””â”€â”€ í…ŒìŠ¤íŠ¸ (Test): 1,000 ìƒ˜í”Œ (8%)
```

**ì¤‘ìš”**: í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ëŠ” í•™ìŠµì— ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ (Data Leakage ë°©ì§€)

---

### 2. **ë‹¤ì–‘í•œ ë‚œì´ë„ì˜ í”„ë¡¬í”„íŠ¸**

#### Easy (ì‰¬ìš´ ì§ˆë¬¸)
```
- "í•œêµ­ì˜ ìˆ˜ë„ëŠ”?"
- "1+1ì€?"
- "ì•ˆë…•í•˜ì„¸ìš”ë¥¼ ì˜ì–´ë¡œ?"
```

#### Medium (ì¤‘ê°„ ë‚œì´ë„)
```
- "ê¹€ì¹˜ì˜ íš¨ëŠ¥ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”."
- "ì„œìš¸ì˜ ì£¼ìš” ê´€ê´‘ì§€ë¥¼ ì¶”ì²œí•´ì£¼ì„¸ìš”."
- "Pythonê³¼ Javaì˜ ì°¨ì´ì ì€?"
```

#### Hard (ì–´ë ¤ìš´ ì§ˆë¬¸)
```
- "ì–‘ìì—­í•™ì˜ ë¶ˆí™•ì •ì„± ì›ë¦¬ë¥¼ ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”."
- "2023ë…„ í•œêµ­ ê²½ì œ ì „ë§ì„ ë¶„ì„í•´ì£¼ì„¸ìš”."
- "ì¸ê³µì§€ëŠ¥ ìœ¤ë¦¬ ë¬¸ì œì— ëŒ€í•œ ë‹¹ì‹ ì˜ ê²¬í•´ëŠ”?"
```

---

## ğŸ“ˆ í‰ê°€ ì‹¤í–‰ ê³„íš

### Phase 1: Baseline í‰ê°€ (í•™ìŠµ ì „)
```bash
python scripts/evaluate.py \
    --model gpt2 \
    --test_data data/processed/test.json \
    --output outputs/eval/baseline_results.json
```

**ì¸¡ì • í•­ëª©**:
- Perplexity
- BLEU Score
- ìƒ˜í”Œ ìƒì„± (20ê°œ í”„ë¡¬í”„íŠ¸)

---

### Phase 2: Fine-tuned í‰ê°€ (í•™ìŠµ í›„)
```bash
python scripts/evaluate.py \
    --model gpt2 \
    --lora_weights outputs/checkpoints/final \
    --test_data data/processed/test.json \
    --output outputs/eval/finetuned_results.json
```

**ì¸¡ì • í•­ëª©**:
- Perplexity (Baseline ëŒ€ë¹„ ê°ì†Œìœ¨)
- BLEU Score (Baseline ëŒ€ë¹„ ì¦ê°€ìœ¨)
- ìƒ˜í”Œ ìƒì„± (ë™ì¼í•œ 20ê°œ í”„ë¡¬í”„íŠ¸)

---

### Phase 3: ë¹„êµ ë¦¬í¬íŠ¸ ìƒì„±
```bash
python scripts/compare_results.py \
    --baseline outputs/eval/baseline_results.json \
    --finetuned outputs/eval/finetuned_results.json \
    --output outputs/eval/comparison_report.html
```

**ë¦¬í¬íŠ¸ ë‚´ìš©**:
1. ë©”íŠ¸ë¦­ ë¹„êµ í‘œ
2. ìƒ˜í”Œ ìƒì„± Side-by-Side ë¹„êµ
3. ê°œì„ ìœ¨ ê·¸ë˜í”„
4. ì‹¤íŒ¨ ì¼€ì´ìŠ¤ ë¶„ì„

---

## ğŸ¯ ì„±ê³µ ê¸°ì¤€ (Success Criteria)

### ìµœì†Œ ëª©í‘œ (Minimum Viable)
- âœ… Perplexity **10% ì´ìƒ ê°ì†Œ**
- âœ… BLEU Score **5ì  ì´ìƒ ì¦ê°€**
- âœ… ìƒ˜í”Œ ìƒì„±ì—ì„œ **ëª…í™•í•œ í’ˆì§ˆ í–¥ìƒ** í™•ì¸

### ê¶Œì¥ ëª©í‘œ (Recommended)
- âœ… Perplexity **20% ì´ìƒ ê°ì†Œ**
- âœ… BLEU Score **10ì  ì´ìƒ ì¦ê°€**
- âœ… Human Evaluationì—ì„œ **í‰ê·  1ì  ì´ìƒ í–¥ìƒ**
- âœ… ë„ë©”ì¸ íŠ¹í™” ì§ˆë¬¸ì—ì„œ **ì •í™•ë„ 30% ì´ìƒ ì¦ê°€**

### ì´ìƒì  ëª©í‘œ (Ideal)
- âœ… Perplexity **30% ì´ìƒ ê°ì†Œ**
- âœ… BLEU Score **15ì  ì´ìƒ ì¦ê°€**
- âœ… Human Evaluationì—ì„œ **80% ì´ìƒ ì„ í˜¸**
- âœ… ë³µì¡í•œ ì§ˆë¬¸ì—ì„œë„ **ì¼ê´€ëœ í’ˆì§ˆ ìœ ì§€**

---

## ğŸ”§ í‰ê°€ ë„êµ¬ êµ¬í˜„

### 1. `scripts/evaluate.py` - ìë™ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
```python
"""
ê¸°ëŠ¥:
- Perplexity ê³„ì‚°
- BLEU/ROUGE ê³„ì‚°
- ìƒ˜í”Œ ìƒì„± ë° ì €ì¥
- ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ì €ì¥
"""
```

### 2. `scripts/compare_results.py` - ë¹„êµ ë¦¬í¬íŠ¸ ìƒì„±
```python
"""
ê¸°ëŠ¥:
- Baseline vs Fine-tuned ë©”íŠ¸ë¦­ ë¹„êµ
- ì‹œê°í™” (ê·¸ë˜í”„, í‘œ)
- HTML ë¦¬í¬íŠ¸ ìƒì„±
- ê°œì„  ì‚¬í•­ í•˜ì´ë¼ì´íŠ¸
"""
```

### 3. `notebooks/evaluation_demo.ipynb` - ì¸í„°ë™í‹°ë¸Œ í‰ê°€
```python
"""
ê¸°ëŠ¥:
- ì‹¤ì‹œê°„ í”„ë¡¬í”„íŠ¸ í…ŒìŠ¤íŠ¸
- Side-by-Side ë¹„êµ
- ìˆ˜ë™ í‰ê°€ ì¸í„°í˜ì´ìŠ¤
"""
```

---

## ğŸ“Š ì˜ˆìƒ ê²°ê³¼ ì˜ˆì‹œ

### Before (Baseline GPT-2)
```json
{
  "perplexity": 45.2,
  "bleu": 12.3,
  "rouge_l": 0.23,
  "sample_quality": "ë³´í†µ",
  "domain_accuracy": 35%
}
```

### After (GPT-2 + LoRA on Korean Instructions)
```json
{
  "perplexity": 28.7,  // â†“ 36.5% ê°ì†Œ
  "bleu": 28.9,        // â†‘ 135% ì¦ê°€
  "rouge_l": 0.41,     // â†‘ 78% ì¦ê°€
  "sample_quality": "ìš°ìˆ˜",
  "domain_accuracy": 72%  // â†‘ 106% ì¦ê°€
}
```

---

## ğŸ’¡ ì¶”ê°€ í‰ê°€ ì•„ì´ë””ì–´

### 1. **í•™ìŠµ ê³¡ì„  ë¶„ì„**
- Epochë³„ ì„±ëŠ¥ ë³€í™” ì¶”ì 
- Overfitting ê°ì§€
- Early Stopping ìµœì  ì‹œì  íŒŒì•…

### 2. **LoRA Rank ì˜í–¥ ë¶„ì„**
- r=4, 8, 16, 32 ë¹„êµ
- ì„±ëŠ¥ vs íŒŒë¼ë¯¸í„° ìˆ˜ íŠ¸ë ˆì´ë“œì˜¤í”„

### 3. **Few-shot vs Zero-shot ë¹„êµ**
- í”„ë¡¬í”„íŠ¸ì— ì˜ˆì‹œ í¬í•¨ ì—¬ë¶€ì— ë”°ë¥¸ ì„±ëŠ¥ ì°¨ì´

### 4. **ë„ë©”ì¸ íŠ¹í™” í‰ê°€**
- í•™ìŠµ ë°ì´í„° ë„ë©”ì¸ì—ì„œì˜ ì„±ëŠ¥
- Out-of-domain ì¼ë°˜í™” ëŠ¥ë ¥

---

## ğŸ¬ ì‹¤í–‰ ìˆœì„œ

```bash
# 1. Baseline í‰ê°€
python scripts/evaluate.py --mode baseline

# 2. í•™ìŠµ ì‹¤í–‰
python scripts/train.py

# 3. Fine-tuned í‰ê°€
python scripts/evaluate.py --mode finetuned

# 4. ë¹„êµ ë¦¬í¬íŠ¸ ìƒì„±
python scripts/compare_results.py

# 5. ë¦¬í¬íŠ¸ í™•ì¸
firefox outputs/eval/comparison_report.html
```

---

## ğŸ“ ìš”ì•½

### í•µì‹¬ í‰ê°€ ì „ëµ
1. **ì •ëŸ‰ì **: Perplexity, BLEU, ROUGE, Accuracy
2. **ì •ì„±ì **: Human Evaluation, Side-by-Side Comparison
3. **ë¹„êµ ê¸°ì¤€**: Baseline (ì›ë³¸) vs Fine-tuned (LoRA ì ìš©)
4. **ì„±ê³µ ì§€í‘œ**: ìµœì†Œ 10-20% ì„±ëŠ¥ í–¥ìƒ

### êµ¬í˜„í•  ìŠ¤í¬ë¦½íŠ¸
- âœ… `evaluate.py` - ìë™ ë©”íŠ¸ë¦­ ê³„ì‚°
- âœ… `compare_results.py` - ë¹„êµ ë¦¬í¬íŠ¸ ìƒì„±
- âœ… `evaluation_demo.ipynb` - ì¸í„°ë™í‹°ë¸Œ í…ŒìŠ¤íŠ¸

ì´ í‰ê°€ ì „ëµì„ í†µí•´ LoRA íŒŒì¸íŠœë‹ì˜ íš¨ê³¼ë¥¼ **ëª…í™•í•˜ê³  ê°ê´€ì ìœ¼ë¡œ** ê²€ì¦í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤! ğŸ¯
