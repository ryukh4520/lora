# LoRA Config 파라미터 심층 분석

## 🎯 목차
1. [Rank (r)의 수학적 의미](#rank-r의-수학적-의미)
2. [LoRA Alpha의 역할](#lora-alpha의-역할)
3. [왜 이렇게 설계되었는가](#왜-이렇게-설계되었는가)
4. [실제 동작 메커니즘](#실제-동작-메커니즘)

---

## 📐 1. Rank (r)의 수학적 의미

### 1.1 선형대수학 관점

#### **Rank의 정의**

```
행렬의 Rank = 선형 독립인 행(또는 열)의 최대 개수
```

**예시**:
```python
# Full rank 행렬
A = [[1, 0, 0],
     [0, 1, 0],
     [0, 0, 1]]
rank(A) = 3  # 모든 행이 독립

# Low rank 행렬
B = [[1, 2, 3],
     [2, 4, 6],
     [3, 6, 9]]
rank(B) = 1  # 모든 행이 첫 번째 행의 배수
```

---

#### **LoRA에서 Rank의 의미**

LoRA는 **가중치 업데이트 ΔW를 Low-Rank로 제한**합니다.

```
원래 가중치 업데이트:
W' = W + ΔW

여기서 ΔW ∈ ℝ^(d×k)는 full rank (최대 min(d,k))

LoRA 가중치 업데이트:
W' = W + B·A

여기서:
- A ∈ ℝ^(r×k)
- B ∈ ℝ^(d×r)
- ΔW = B·A의 rank는 최대 r
```

---

### 1.2 왜 Rank를 제한하는가?

#### **핵심 가정: Low-Rank Hypothesis**

```
"대부분의 딥러닝 모델에서 가중치 업데이트는 
 실제로 낮은 차원의 부분공간(subspace)에서 일어난다"
```

**증명 (실험적)**:
```python
# 연구 결과 (LoRA 논문)
Full Fine-tuning 후 ΔW 분석:
- ΔW의 특이값 분해(SVD): ΔW = U·Σ·V^T
- 상위 r개 특이값이 전체 에너지의 90%+ 차지
- 나머지 특이값은 거의 0에 가까움

결론:
→ ΔW는 사실상 low-rank!
→ r개의 차원만으로도 충분히 표현 가능
```

---

### 1.3 Rank의 기하학적 의미

#### **고차원 공간에서의 부분공간**

```
원래 가중치 공간: ℝ^(768×768) = 589,824 차원

LoRA는 이 중 r차원 부분공간만 사용:
- r=8: 8차원 부분공간
- r=16: 16차원 부분공간
- r=32: 32차원 부분공간

시각화 (3D 예시):
┌─────────────────────────────────────┐
│  전체 공간 (ℝ³)                     │
│                                     │
│      ┌──────────┐                  │
│      │          │                  │
│      │  r=2     │  ← 2차원 평면   │
│      │ 부분공간  │                  │
│      │          │                  │
│      └──────────┘                  │
│                                     │
└─────────────────────────────────────┘

r이 작을수록:
- 더 제한된 부분공간
- 더 적은 "자유도"
- 더 간단한 변환만 가능

r이 클수록:
- 더 넓은 부분공간
- 더 많은 "자유도"
- 더 복잡한 변환 가능
```

---

### 1.4 왜 복잡도에 따라 r이 달라지는가?

#### **정보 이론 관점**

```python
# 태스크의 "본질적 차원(Intrinsic Dimensionality)"

간단한 태스크:
예: "긍정/부정 분류"
→ 본질적 차원: 낮음 (1-2차원)
→ 필요한 r: 4-8

복잡한 태스크:
예: "창의적 글쓰기"
→ 본질적 차원: 높음 (수십 차원)
→ 필요한 r: 16-32
```

**수학적 설명**:
```
태스크를 학습하는데 필요한 최소 차원 = 본질적 차원

간단한 태스크:
f(x) = w₁·x₁ + w₂·x₂  (2차원)
→ r=4로 충분

복잡한 태스크:
f(x) = Σᵢ₌₁³² wᵢ·φᵢ(x)  (32차원)
→ r=32 필요
```

---

### 1.5 왜 데이터 크기에 따라 r이 달라지는가?

#### **과적합(Overfitting) 관점**

```python
# 파라미터 수 vs 데이터 수

작은 데이터 (< 1K):
파라미터가 많으면 → Overfitting
→ r을 작게 (4-8)
→ 정규화 효과

큰 데이터 (> 100K):
파라미터가 많아도 → 충분히 학습 가능
→ r을 크게 (32-64)
→ 표현력 증가
```

**수학적 근거**:
```
VC 차원 (Vapnik-Chervonenkis Dimension):
모델의 복잡도 ∝ 파라미터 수

필요한 데이터 수 ∝ VC 차원

r=8:  파라미터 ~12K  → 데이터 ~1K
r=16: 파라미터 ~24K  → 데이터 ~10K
r=32: 파라미터 ~48K  → 데이터 ~100K
```

---

## 🎚️ 2. LoRA Alpha의 역할

### 2.1 Alpha의 수학적 정의

```python
# Forward pass
output = W @ x + (B @ A @ x) * (lora_alpha / r)
                                 ↑
                            scaling factor
```

**Alpha의 역할**:
```
lora_alpha / r = LoRA 출력의 "크기" 조절

alpha가 클수록:
→ LoRA의 영향력 증가
→ 원래 가중치 W보다 ΔW의 영향이 커짐

alpha가 작을수록:
→ LoRA의 영향력 감소
→ 원래 가중치 W의 영향이 더 큼
```

---

### 2.2 왜 Alpha가 필요한가?

#### **문제: Rank에 따른 스케일 변화**

```python
# r이 다를 때 ΔW의 크기 비교

r=4:
ΔW = B @ A
B: (768, 4), A: (4, 768)
→ ΔW의 "크기"가 작음

r=32:
ΔW = B @ A
B: (768, 32), A: (32, 768)
→ ΔW의 "크기"가 큼

문제:
r을 바꾸면 ΔW의 스케일이 변함
→ 학습률, 수렴 속도가 달라짐
→ 하이퍼파라미터 튜닝이 어려움
```

---

#### **해결: Alpha로 정규화**

```python
# Alpha 없이
output = W @ x + B @ A @ x
→ r에 따라 스케일 변화

# Alpha 사용
output = W @ x + (B @ A @ x) * (alpha / r)
→ r이 바뀌어도 스케일 유지 가능
```

**예시**:
```python
# alpha = r * 2로 설정 (일반적)

r=4,  alpha=8:   scaling = 8/4  = 2.0
r=8,  alpha=16:  scaling = 16/8 = 2.0
r=16, alpha=32:  scaling = 32/16 = 2.0
r=32, alpha=64:  scaling = 64/32 = 2.0

→ r이 바뀌어도 scaling은 2.0으로 일정!
→ 학습 안정성 유지
```

---

### 2.3 Alpha의 초기화 효과

#### **초기 상태 분석**

```python
# 초기화
A ~ N(0, σ²)  # Gaussian
B = 0         # Zero

# 초기 ΔW
ΔW = B @ A = 0 @ A = 0

# 초기 출력
output = W @ x + ΔW @ x * (alpha/r)
       = W @ x + 0
       = W @ x

→ 학습 시작 시 원래 모델과 동일!
```

**학습 진행**:
```python
# Epoch 1
B가 0에서 벗어남
ΔW = B @ A ≠ 0
output = W @ x + ΔW @ x * (alpha/r)

alpha가 클수록:
→ ΔW의 영향이 빠르게 증가
→ 빠른 수렴

alpha가 작을수록:
→ ΔW의 영향이 천천히 증가
→ 안정적 학습
```

---

### 2.4 Alpha의 학습률 효과

#### **Effective Learning Rate**

```python
# 실제 학습률 = lr × (alpha / r)

lr = 2e-4, r = 8, alpha = 16:
effective_lr = 2e-4 × (16/8) = 2e-4 × 2 = 4e-4

lr = 2e-4, r = 8, alpha = 8:
effective_lr = 2e-4 × (8/8) = 2e-4 × 1 = 2e-4

lr = 2e-4, r = 8, alpha = 32:
effective_lr = 2e-4 × (32/8) = 2e-4 × 4 = 8e-4
```

**의미**:
```
alpha를 조정 = 학습률을 조정하는 것과 유사

alpha 증가 → 빠른 학습, 불안정 위험
alpha 감소 → 느린 학습, 안정적
```

---

## 🔬 3. 왜 이렇게 설계되었는가?

### 3.1 설계 목표

```
1. 파라미터 효율성
   → Low-rank decomposition (r)

2. 학습 안정성
   → Scaling factor (alpha/r)

3. 하이퍼파라미터 독립성
   → r을 바꿔도 alpha 조정으로 보상

4. 초기화 안정성
   → B=0으로 시작, alpha로 점진적 영향
```

---

### 3.2 수학적 근거

#### **특이값 분해(SVD) 관점**

```python
# 가중치 업데이트의 SVD
ΔW = U @ Σ @ V^T

여기서:
- U, V: 직교 행렬
- Σ: 특이값 대각 행렬

Σ = diag(σ₁, σ₂, ..., σₙ)
σ₁ ≥ σ₂ ≥ ... ≥ σₙ ≥ 0

Low-rank approximation:
ΔW ≈ Σᵢ₌₁ʳ σᵢ · uᵢ · vᵢ^T

LoRA는 이를 학습 가능하게 만듦:
ΔW = B @ A
→ B, A를 학습하면서 자동으로 최적의 low-rank 근사 찾기
```

---

#### **정보 압축 관점**

```
원래 정보: 768×768 = 589,824 bits

LoRA (r=8):
- A: 8×768 = 6,144 bits
- B: 768×8 = 6,144 bits
- 합계: 12,288 bits

압축률: 12,288 / 589,824 = 2.08%

r이 작을수록:
→ 더 높은 압축률
→ 더 적은 정보 저장
→ 간단한 패턴만 학습

r이 클수록:
→ 더 낮은 압축률
→ 더 많은 정보 저장
→ 복잡한 패턴 학습
```

---

## ⚙️ 4. 실제 동작 메커니즘

### 4.1 Forward Pass 상세

```python
# 입력
x: (batch, 768)

# Step 1: 원래 레이어
h_base = W @ x
W: (768, 768), frozen ❄️
h_base: (batch, 768)

# Step 2: LoRA A
temp = A @ x
A: (8, 768), trainable 🔥
temp: (batch, 8)  ← 8차원으로 압축!

# Step 3: LoRA B
h_lora = B @ temp
B: (768, 8), trainable 🔥
h_lora: (batch, 768)  ← 다시 768차원으로 확장

# Step 4: Scaling
h_lora = h_lora * (alpha / r)
h_lora = h_lora * (16 / 8)
h_lora = h_lora * 2.0

# Step 5: 결합
h = h_base + h_lora
h: (batch, 768)
```

**핵심**:
```
768차원 → 8차원 → 768차원

8차원 "병목(bottleneck)"이 정보 압축/추출의 핵심!

r=8:  8차원 병목
r=16: 16차원 병목
r=32: 32차원 병목

병목이 클수록 더 많은 정보 통과 가능
```

---

### 4.2 우리 프로젝트 실제 예시

```python
# 설정
r = 8
lora_alpha = 16
scaling = 16 / 8 = 2.0

# GPT-2 c_attn (768 → 2304)
A: (8, 768)   = 6,144 params
B: (2304, 8)  = 18,432 params
Total: 24,576 params

# Forward (batch_size=1, seq_len=512)
x: (1, 512, 768)

# 각 토큰마다:
for each token in sequence:
    x_token: (1, 768)
    
    # 원래
    h_base = W @ x_token  # (1, 2304)
    
    # LoRA
    temp = A @ x_token    # (1, 8) ← 압축!
    h_lora = B @ temp     # (1, 2304) ← 확장!
    h_lora = h_lora * 2.0 # Scaling
    
    # 결합
    h = h_base + h_lora   # (1, 2304)
```

---

## 🎯 핵심 요약

### **Rank (r)**

```
수학적 의미:
- 가중치 업데이트 ΔW의 차원
- 정보가 통과하는 "병목" 크기
- 학습 가능한 부분공간의 차원

왜 복잡도에 따라 다른가:
- 간단한 태스크 = 낮은 본질적 차원 → 작은 r
- 복잡한 태스크 = 높은 본질적 차원 → 큰 r

왜 데이터 크기에 따라 다른가:
- 작은 데이터 = Overfitting 위험 → 작은 r (정규화)
- 큰 데이터 = 충분한 학습 → 큰 r (표현력)

우리 선택 (r=8):
- 1,000 샘플 (작음)
- 간단한 QA
- 8차원 부분공간으로 충분
```

---

### **LoRA Alpha**

```
수학적 의미:
- LoRA 출력의 스케일 조절
- Effective learning rate 조정
- r에 따른 스케일 변화 보상

왜 필요한가:
- r이 바뀌면 ΔW 크기 변함
- alpha로 정규화하여 일관성 유지
- 학습 안정성 확보

설정 기준:
- 일반적: alpha = r × 2
- 보수적: alpha = r
- 공격적: alpha = r × 4

우리 선택 (alpha=16):
- r=8의 2배 (표준)
- scaling = 2.0
- 안정적이면서 빠른 수렴
```

---

이제 `r`과 `lora_alpha`의 **진짜 의미**를 완전히 이해하셨을 것입니다! 🚀
