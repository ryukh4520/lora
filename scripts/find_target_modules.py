"""
LoRA Target Module íƒìƒ‰ ìœ í‹¸ë¦¬í‹°

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ëª¨ë¸ì˜ êµ¬ì¡°ë¥¼ ë¶„ì„í•˜ê³  LoRA ì ìš© ê°€ëŠ¥í•œ ëª¨ë“ˆì„ ì°¾ìŠµë‹ˆë‹¤.
"""

import torch
from transformers import AutoModelForCausalLM, AutoModel
from transformers.pytorch_utils import Conv1D
from collections import defaultdict


def find_all_linear_layers(model, verbose=True):
    """
    ëª¨ë¸ì˜ ëª¨ë“  Linear ë ˆì´ì–´ ì°¾ê¸°
    
    Args:
        model: Hugging Face ëª¨ë¸
        verbose: ìƒì„¸ ì¶œë ¥ ì—¬ë¶€
    
    Returns:
        dict: {module_name: [layer_names]}
    """
    linear_layers = defaultdict(list)
    
    for name, module in model.named_modules():
        # Linear ë˜ëŠ” Conv1D (GPT-2ì˜ ê²½ìš°)
        if isinstance(module, (torch.nn.Linear, Conv1D)):
            # ëª¨ë“ˆ ì´ë¦„ ì¶”ì¶œ (ë§ˆì§€ë§‰ ë¶€ë¶„)
            module_name = name.split('.')[-1]
            linear_layers[module_name].append({
                'full_name': name,
                'shape': tuple(module.weight.shape),
                'params': module.weight.numel()
            })
    
    if verbose:
        print("=" * 80)
        print("ğŸ“Š Linear Layer ë¶„ì„ ê²°ê³¼")
        print("=" * 80)
        
        for module_name, layers in sorted(linear_layers.items()):
            print(f"\nğŸ”¹ {module_name}: {len(layers)}ê°œ")
            if layers:
                first_layer = layers[0]
                print(f"   Shape: {first_layer['shape']}")
                print(f"   Params: {first_layer['params']:,}")
                print(f"   ì˜ˆì‹œ: {first_layer['full_name']}")
    
    return dict(linear_layers)


def analyze_attention_modules(model, verbose=True):
    """
    Attention ê´€ë ¨ ëª¨ë“ˆ ë¶„ì„
    
    Args:
        model: Hugging Face ëª¨ë¸
        verbose: ìƒì„¸ ì¶œë ¥ ì—¬ë¶€
    
    Returns:
        list: Attention ëª¨ë“ˆ ì´ë¦„ ë¦¬ìŠ¤íŠ¸
    """
    attention_modules = set()
    
    for name, module in model.named_modules():
        # 'attn' ë˜ëŠ” 'attention'ì´ ì´ë¦„ì— í¬í•¨ëœ ê²½ìš°
        if 'attn' in name.lower() or 'attention' in name.lower():
            if isinstance(module, (torch.nn.Linear, Conv1D)):
                module_name = name.split('.')[-1]
                attention_modules.add(module_name)
    
    if verbose:
        print("\n" + "=" * 80)
        print("ğŸ¯ Attention ëª¨ë“ˆ")
        print("=" * 80)
        for module in sorted(attention_modules):
            print(f"  âœ… {module}")
    
    return sorted(attention_modules)


def analyze_mlp_modules(model, verbose=True):
    """
    MLP/FFN ê´€ë ¨ ëª¨ë“ˆ ë¶„ì„
    
    Args:
        model: Hugging Face ëª¨ë¸
        verbose: ìƒì„¸ ì¶œë ¥ ì—¬ë¶€
    
    Returns:
        list: MLP ëª¨ë“ˆ ì´ë¦„ ë¦¬ìŠ¤íŠ¸
    """
    mlp_modules = set()
    
    for name, module in model.named_modules():
        # 'mlp' ë˜ëŠ” 'ffn'ì´ ì´ë¦„ì— í¬í•¨ëœ ê²½ìš°
        if 'mlp' in name.lower() or 'ffn' in name.lower() or 'feed_forward' in name.lower():
            if isinstance(module, (torch.nn.Linear, Conv1D)):
                module_name = name.split('.')[-1]
                mlp_modules.add(module_name)
    
    if verbose:
        print("\n" + "=" * 80)
        print("ğŸ”§ MLP/FFN ëª¨ë“ˆ")
        print("=" * 80)
        for module in sorted(mlp_modules):
            print(f"  âœ… {module}")
    
    return sorted(mlp_modules)


def suggest_target_modules(model, strategy='attention_only'):
    """
    ëª¨ë¸ì— ì í•©í•œ target_modules ì œì•ˆ
    
    Args:
        model: Hugging Face ëª¨ë¸
        strategy: 'attention_only', 'attention_mlp', 'efficient'
    
    Returns:
        list: ê¶Œì¥ target_modules
    """
    all_layers = find_all_linear_layers(model, verbose=False)
    attn_modules = analyze_attention_modules(model, verbose=False)
    mlp_modules = analyze_mlp_modules(model, verbose=False)
    
    print("\n" + "=" * 80)
    print("ğŸ’¡ Target Modules ì œì•ˆ")
    print("=" * 80)
    
    if strategy == 'attention_only':
        print("\nì „ëµ: Attention Only (ê¸°ë³¸, ê¶Œì¥)")
        print("ì¥ì : íš¨ìœ¨ì , ëŒ€ë¶€ë¶„ì˜ ê²½ìš° ì¶©ë¶„")
        target = attn_modules
        
    elif strategy == 'attention_mlp':
        print("\nì „ëµ: Attention + MLP (ë†’ì€ ì„±ëŠ¥)")
        print("ì¥ì : ë†’ì€ í‘œí˜„ë ¥, ë³µì¡í•œ íƒœìŠ¤í¬ ëŒ€ì‘")
        print("ë‹¨ì : íŒŒë¼ë¯¸í„° 2-3ë°° ì¦ê°€")
        target = attn_modules + mlp_modules
        
    elif strategy == 'efficient':
        print("\nì „ëµ: Efficient (ë©”ëª¨ë¦¬ ì œì•½)")
        print("ì¥ì : íŒŒë¼ë¯¸í„° ì ˆì•½")
        # Query, Valueë§Œ (ì¼ë°˜ì ìœ¼ë¡œ q_proj, v_proj ë˜ëŠ” query, value)
        target = [m for m in attn_modules if 'q' in m.lower() or 'v' in m.lower() or 'query' in m.lower() or 'value' in m.lower()]
        if not target:
            target = attn_modules[:2]  # ì²˜ìŒ 2ê°œë§Œ
    
    else:
        raise ValueError(f"Unknown strategy: {strategy}")
    
    print(f"\nê¶Œì¥ target_modules:")
    for module in target:
        print(f"  - {module}")
    
    return target


def estimate_lora_params(model, target_modules, r=8):
    """
    LoRA ì ìš© ì‹œ íŒŒë¼ë¯¸í„° ìˆ˜ ì¶”ì •
    
    Args:
        model: Hugging Face ëª¨ë¸
        target_modules: Target module ë¦¬ìŠ¤íŠ¸
        r: LoRA rank
    
    Returns:
        dict: íŒŒë¼ë¯¸í„° í†µê³„
    """
    total_lora_params = 0
    layer_count = 0
    
    for name, module in model.named_modules():
        if isinstance(module, (torch.nn.Linear, Conv1D)):
            module_name = name.split('.')[-1]
            if module_name in target_modules:
                # LoRA íŒŒë¼ë¯¸í„°: r * (in_features + out_features)
                in_features = module.weight.shape[1]
                out_features = module.weight.shape[0]
                lora_params = r * (in_features + out_features)
                total_lora_params += lora_params
                layer_count += 1
    
    # ì „ì²´ ëª¨ë¸ íŒŒë¼ë¯¸í„°
    total_params = sum(p.numel() for p in model.parameters())
    
    stats = {
        'total_params': total_params,
        'lora_params': total_lora_params,
        'trainable_ratio': 100 * total_lora_params / total_params,
        'layer_count': layer_count
    }
    
    print("\n" + "=" * 80)
    print("ğŸ“ˆ LoRA íŒŒë¼ë¯¸í„° ì¶”ì •")
    print("=" * 80)
    print(f"ì „ì²´ íŒŒë¼ë¯¸í„°:     {stats['total_params']:,}")
    print(f"LoRA íŒŒë¼ë¯¸í„°:     {stats['lora_params']:,}")
    print(f"í•™ìŠµ ë¹„ìœ¨:         {stats['trainable_ratio']:.4f}%")
    print(f"ì ìš© ë ˆì´ì–´ ìˆ˜:    {stats['layer_count']}")
    
    return stats


def verify_target_modules(model, target_modules):
    """
    Target modulesê°€ ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    
    Args:
        model: Hugging Face ëª¨ë¸
        target_modules: í™•ì¸í•  module ë¦¬ìŠ¤íŠ¸
    
    Returns:
        dict: ê²€ì¦ ê²°ê³¼
    """
    all_modules = set()
    matched_layers = []
    
    for name, module in model.named_modules():
        if isinstance(module, (torch.nn.Linear, Conv1D)):
            module_name = name.split('.')[-1]
            all_modules.add(module_name)
            if module_name in target_modules:
                matched_layers.append(name)
    
    print("\n" + "=" * 80)
    print("âœ… Target Modules ê²€ì¦")
    print("=" * 80)
    
    for target in target_modules:
        if target in all_modules:
            count = sum(1 for name in matched_layers if name.endswith(target))
            print(f"  âœ… {target}: ì¡´ì¬í•¨ ({count}ê°œ ë ˆì´ì–´)")
        else:
            print(f"  âŒ {target}: ì¡´ì¬í•˜ì§€ ì•ŠìŒ!")
    
    if not all(t in all_modules for t in target_modules):
        print(f"\nì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“ˆ:")
        for module in sorted(all_modules):
            print(f"  - {module}")
    
    return {
        'valid': all(t in all_modules for t in target_modules),
        'matched_count': len(matched_layers),
        'available_modules': sorted(all_modules)
    }


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description='LoRA Target Module íƒìƒ‰')
    parser.add_argument('--model', type=str, default='gpt2',
                       help='ëª¨ë¸ ì´ë¦„ (ì˜ˆ: gpt2, meta-llama/Llama-2-7b-hf)')
    parser.add_argument('--strategy', type=str, default='attention_only',
                       choices=['attention_only', 'attention_mlp', 'efficient'],
                       help='Target module ì„ ì • ì „ëµ')
    parser.add_argument('--r', type=int, default=8,
                       help='LoRA rank')
    parser.add_argument('--verify', type=str, nargs='+',
                       help='ê²€ì¦í•  target modules (ì˜ˆ: --verify c_attn c_proj)')
    
    args = parser.parse_args()
    
    print(f"\nğŸ” ëª¨ë¸ ë¶„ì„ ì¤‘: {args.model}")
    print("=" * 80)
    
    # ëª¨ë¸ ë¡œë“œ
    try:
        model = AutoModelForCausalLM.from_pretrained(
            args.model,
            trust_remote_code=True
        )
    except:
        try:
            model = AutoModel.from_pretrained(
                args.model,
                trust_remote_code=True
            )
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return
    
    # ë¶„ì„
    find_all_linear_layers(model)
    analyze_attention_modules(model)
    analyze_mlp_modules(model)
    
    # ì œì•ˆ
    target_modules = suggest_target_modules(model, args.strategy)
    
    # íŒŒë¼ë¯¸í„° ì¶”ì •
    estimate_lora_params(model, target_modules, args.r)
    
    # ê²€ì¦ (ì‚¬ìš©ì ì§€ì • ì‹œ)
    if args.verify:
        verify_target_modules(model, args.verify)
    
    # ì„¤ì • ì¶œë ¥
    print("\n" + "=" * 80)
    print("ğŸ“ LoRA ì„¤ì • ì˜ˆì‹œ")
    print("=" * 80)
    print(f"""
lora_config = {{
    "r": {args.r},
    "lora_alpha": {args.r * 2},
    "target_modules": {target_modules},
    "lora_dropout": 0.05,
    "bias": "none",
    "task_type": "CAUSAL_LM"
}}
""")


if __name__ == "__main__":
    main()
