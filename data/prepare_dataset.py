"""
Data preparation script for LoRA fine-tuning.

This script downloads and prepares datasets for training.
Supports:
- KoAlpaca dataset (Korean instruction-following)
- Custom JSON datasets
- Sample/demo datasets
"""

import os
import json
import argparse
import random
from typing import List, Dict
from datasets import load_dataset


def create_sample_dataset(num_samples: int = 100) -> List[Dict]:
    """
    Create a sample dataset for testing.
    
    Args:
        num_samples: Number of samples to generate
        
    Returns:
        List of instruction-output pairs
    """
    print(f"ğŸ“ Creating sample dataset with {num_samples} samples...")
    
    # Sample Korean instruction-following data
    samples = [
        {
            "instruction": "í•œêµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?",
            "input": "",
            "output": "í•œêµ­ì˜ ìˆ˜ë„ëŠ” ì„œìš¸ì…ë‹ˆë‹¤. ì„œìš¸ì€ ëŒ€í•œë¯¼êµ­ì˜ ì •ì¹˜, ê²½ì œ, ë¬¸í™”ì˜ ì¤‘ì‹¬ì§€ì´ë©°, ì•½ 1ì²œë§Œ ëª…ì˜ ì¸êµ¬ê°€ ê±°ì£¼í•˜ê³  ìˆìŠµë‹ˆë‹¤."
        },
        {
            "instruction": "ê¹€ì¹˜ì˜ íš¨ëŠ¥ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
            "input": "",
            "output": "ê¹€ì¹˜ëŠ” í•œêµ­ì˜ ëŒ€í‘œì ì¸ ë°œíš¨ ìŒì‹ìœ¼ë¡œ, ìœ ì‚°ê· ì´ í’ë¶€í•˜ì—¬ ì¥ ê±´ê°•ì— ì¢‹ìŠµë‹ˆë‹¤. ë˜í•œ ë¹„íƒ€ë¯¼ Cì™€ ì‹ì´ì„¬ìœ ê°€ í’ë¶€í•˜ë©°, í•­ì‚°í™” íš¨ê³¼ë„ ìˆì–´ ë©´ì—­ë ¥ í–¥ìƒì— ë„ì›€ì´ ë©ë‹ˆë‹¤."
        },
        {
            "instruction": "ë‹¤ìŒ ë¬¸ì¥ì„ ì˜ì–´ë¡œ ë²ˆì—­í•˜ì„¸ìš”.",
            "input": "ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì •ë§ ì¢‹ë„¤ìš”.",
            "output": "The weather is really nice today."
        },
        {
            "instruction": "Pythonì—ì„œ ë¦¬ìŠ¤íŠ¸ì™€ íŠœí”Œì˜ ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?",
            "input": "",
            "output": "ë¦¬ìŠ¤íŠ¸(list)ëŠ” ë³€ê²½ ê°€ëŠ¥í•œ(mutable) ìë£Œêµ¬ì¡°ë¡œ, ìš”ì†Œë¥¼ ì¶”ê°€, ì‚­ì œ, ìˆ˜ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë°˜ë©´ íŠœí”Œ(tuple)ì€ ë³€ê²½ ë¶ˆê°€ëŠ¥í•œ(immutable) ìë£Œêµ¬ì¡°ë¡œ, í•œ ë²ˆ ìƒì„±ë˜ë©´ ë‚´ìš©ì„ ë³€ê²½í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¦¬ìŠ¤íŠ¸ëŠ” ëŒ€ê´„í˜¸ []ë¡œ, íŠœí”Œì€ ì†Œê´„í˜¸ ()ë¡œ í‘œí˜„í•©ë‹ˆë‹¤."
        },
        {
            "instruction": "ì„œìš¸ì˜ ì£¼ìš” ê´€ê´‘ì§€ë¥¼ ì¶”ì²œí•´ì£¼ì„¸ìš”.",
            "input": "",
            "output": "ì„œìš¸ì˜ ì£¼ìš” ê´€ê´‘ì§€ë¡œëŠ” ê²½ë³µê¶, ë‚¨ì‚°íƒ€ì›Œ, ëª…ë™, ì¸ì‚¬ë™, í™ëŒ€, ê°•ë‚¨ ë“±ì´ ìˆìŠµë‹ˆë‹¤. ê²½ë³µê¶ì€ ì¡°ì„ ì‹œëŒ€ì˜ ëŒ€í‘œì ì¸ ê¶ê¶ì´ë©°, ë‚¨ì‚°íƒ€ì›Œì—ì„œëŠ” ì„œìš¸ ì „ê²½ì„ ê°ìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ëª…ë™ê³¼ í™ëŒ€ëŠ” ì‡¼í•‘ê³¼ ë¨¹ê±°ë¦¬ë¡œ ìœ ëª…í•˜ê³ , ì¸ì‚¬ë™ì€ ì „í†µ ë¬¸í™”ë¥¼ ì²´í—˜í•  ìˆ˜ ìˆëŠ” ê³³ì…ë‹ˆë‹¤."
        },
        {
            "instruction": "ì¸ê³µì§€ëŠ¥ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?",
            "input": "",
            "output": "ì¸ê³µì§€ëŠ¥(AI, Artificial Intelligence)ì€ ì¸ê°„ì˜ í•™ìŠµëŠ¥ë ¥, ì¶”ë¡ ëŠ¥ë ¥, ì§€ê°ëŠ¥ë ¥ ë“±ì„ ì»´í“¨í„° í”„ë¡œê·¸ë¨ìœ¼ë¡œ êµ¬í˜„í•œ ê¸°ìˆ ì…ë‹ˆë‹¤. ë¨¸ì‹ ëŸ¬ë‹, ë”¥ëŸ¬ë‹ ë“±ì˜ ê¸°ìˆ ì„ í†µí•´ ë°ì´í„°ë¡œë¶€í„° íŒ¨í„´ì„ í•™ìŠµí•˜ê³ , ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì˜ˆì¸¡ì´ë‚˜ ì˜ì‚¬ê²°ì •ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."
        },
        {
            "instruction": "ë‹¤ìŒ ìˆ«ìë“¤ì˜ í‰ê· ì„ ê³„ì‚°í•˜ì„¸ìš”.",
            "input": "10, 20, 30, 40, 50",
            "output": "ì£¼ì–´ì§„ ìˆ«ìë“¤ì˜ í‰ê· ì€ 30ì…ë‹ˆë‹¤. ê³„ì‚° ê³¼ì •: (10 + 20 + 30 + 40 + 50) / 5 = 150 / 5 = 30"
        },
        {
            "instruction": "ê±´ê°•í•œ ì•„ì¹¨ ì‹ì‚¬ ë©”ë‰´ë¥¼ ì¶”ì²œí•´ì£¼ì„¸ìš”.",
            "input": "",
            "output": "ê±´ê°•í•œ ì•„ì¹¨ ì‹ì‚¬ë¡œëŠ” í†µê³¡ë¬¼ ë¹µ, ê³„ë€, ìƒëŸ¬ë“œ, ê³¼ì¼, ìš°ìœ  ë˜ëŠ” ìš”ê±°íŠ¸ë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤. í†µê³¡ë¬¼ì€ ì‹ì´ì„¬ìœ ê°€ í’ë¶€í•˜ê³ , ê³„ë€ì€ ì–‘ì§ˆì˜ ë‹¨ë°±ì§ˆì„ ì œê³µí•©ë‹ˆë‹¤. ê³¼ì¼ê³¼ ì±„ì†ŒëŠ” ë¹„íƒ€ë¯¼ê³¼ ë¯¸ë„¤ë„ì„ ê³µê¸‰í•˜ë©°, ìœ ì œí’ˆì€ ì¹¼ìŠ˜ì„ ë³´ì¶©í•´ì¤ë‹ˆë‹¤."
        },
        {
            "instruction": "í™˜ê²½ ë³´í˜¸ë¥¼ ìœ„í•´ ê°œì¸ì´ í•  ìˆ˜ ìˆëŠ” ì¼ì€ ë¬´ì—‡ì¸ê°€ìš”?",
            "input": "",
            "output": "ê°œì¸ì´ í™˜ê²½ ë³´í˜¸ë¥¼ ìœ„í•´ í•  ìˆ˜ ìˆëŠ” ì¼ë¡œëŠ” ì¼íšŒìš©í’ˆ ì‚¬ìš© ì¤„ì´ê¸°, ë¶„ë¦¬ìˆ˜ê±° ì² ì €íˆ í•˜ê¸°, ëŒ€ì¤‘êµí†µ ì´ìš©í•˜ê¸°, ì—ë„ˆì§€ ì ˆì•½í•˜ê¸°, ì¹œí™˜ê²½ ì œí’ˆ ì‚¬ìš©í•˜ê¸° ë“±ì´ ìˆìŠµë‹ˆë‹¤. ì‘ì€ ì‹¤ì²œë“¤ì´ ëª¨ì—¬ í° ë³€í™”ë¥¼ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        },
        {
            "instruction": "ìŠ¤íŠ¸ë ˆìŠ¤ í•´ì†Œ ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”.",
            "input": "",
            "output": "ìŠ¤íŠ¸ë ˆìŠ¤ í•´ì†Œ ë°©ë²•ìœ¼ë¡œëŠ” ê·œì¹™ì ì¸ ìš´ë™, ì¶©ë¶„í•œ ìˆ˜ë©´, ëª…ìƒì´ë‚˜ ìš”ê°€, ì·¨ë¯¸ í™œë™, ì¹œêµ¬ë‚˜ ê°€ì¡±ê³¼ì˜ ëŒ€í™” ë“±ì´ ìˆìŠµë‹ˆë‹¤. ë˜í•œ ê¹Šì€ í˜¸í¡ì´ë‚˜ ì‚°ì±…ë„ ì¦‰ê°ì ì¸ ìŠ¤íŠ¸ë ˆìŠ¤ ì™„í™”ì— ë„ì›€ì´ ë©ë‹ˆë‹¤."
        }
    ]
    
    # Repeat samples to reach desired number
    dataset = []
    while len(dataset) < num_samples:
        dataset.extend(samples)
    
    # Shuffle and trim to exact number
    random.shuffle(dataset)
    dataset = dataset[:num_samples]
    
    print(f"âœ… Created {len(dataset)} sample entries")
    return dataset


def download_koalpaca_dataset(num_samples: int = None) -> List[Dict]:
    """
    Download KoAlpaca dataset from Hugging Face.
    
    Args:
        num_samples: Number of samples to use (None for all)
        
    Returns:
        List of instruction-output pairs
    """
    print("ğŸ“¥ Downloading KoAlpaca dataset from Hugging Face...")
    
    try:
        # Load KoAlpaca dataset
        dataset = load_dataset("beomi/KoAlpaca-v1.1a", split="train")
        
        # Convert to our format
        data = []
        for item in dataset:
            data.append({
                "instruction": item.get("instruction", ""),
                "input": item.get("input", ""),
                "output": item.get("output", "")
            })
        
        # Limit samples if specified
        if num_samples and num_samples < len(data):
            random.shuffle(data)
            data = data[:num_samples]
        
        print(f"âœ… Downloaded {len(data)} samples from KoAlpaca")
        return data
    
    except Exception as e:
        print(f"âŒ Failed to download KoAlpaca: {e}")
        print("ğŸ’¡ Falling back to sample dataset...")
        return create_sample_dataset(num_samples or 1000)


def load_custom_dataset(file_path: str) -> List[Dict]:
    """
    Load custom dataset from JSON file.
    
    Args:
        file_path: Path to JSON file
        
    Returns:
        List of instruction-output pairs
    """
    print(f"ğŸ“‚ Loading custom dataset from {file_path}...")
    
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    print(f"âœ… Loaded {len(data)} samples")
    return data


def split_dataset(
    data: List[Dict],
    train_ratio: float = 0.8,
    val_ratio: float = 0.1,
    test_ratio: float = 0.1,
    shuffle: bool = True
) -> tuple:
    """
    Split dataset into train, validation, and test sets.
    
    Args:
        data: Full dataset
        train_ratio: Ratio for training set
        val_ratio: Ratio for validation set
        test_ratio: Ratio for test set
        shuffle: Whether to shuffle before splitting
        
    Returns:
        Tuple of (train_data, val_data, test_data)
    """
    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6, \
        "Ratios must sum to 1.0"
    
    if shuffle:
        random.shuffle(data)
    
    total = len(data)
    train_end = int(total * train_ratio)
    val_end = train_end + int(total * val_ratio)
    
    train_data = data[:train_end]
    val_data = data[train_end:val_end]
    test_data = data[val_end:]
    
    print(f"\nğŸ“Š Dataset Split:")
    print(f"   Train:      {len(train_data):5d} samples ({len(train_data)/total*100:.1f}%)")
    print(f"   Validation: {len(val_data):5d} samples ({len(val_data)/total*100:.1f}%)")
    print(f"   Test:       {len(test_data):5d} samples ({len(test_data)/total*100:.1f}%)")
    print(f"   Total:      {total:5d} samples")
    
    return train_data, val_data, test_data


def save_dataset(data: List[Dict], output_path: str):
    """
    Save dataset to JSON file.
    
    Args:
        data: Dataset to save
        output_path: Output file path
    """
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… Saved {len(data)} samples to {output_path}")


def main():
    parser = argparse.ArgumentParser(description="Prepare dataset for LoRA fine-tuning")
    parser.add_argument(
        "--dataset",
        type=str,
        default="sample",
        choices=["sample", "koalpaca", "custom"],
        help="Dataset type to use"
    )
    parser.add_argument(
        "--custom_file",
        type=str,
        help="Path to custom dataset file (required if dataset=custom)"
    )
    parser.add_argument(
        "--num_samples",
        type=int,
        default=1000,
        help="Number of samples to use (for sample/koalpaca)"
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        default="data/processed",
        help="Output directory for processed data"
    )
    parser.add_argument(
        "--train_ratio",
        type=float,
        default=0.8,
        help="Training set ratio"
    )
    parser.add_argument(
        "--val_ratio",
        type=float,
        default=0.1,
        help="Validation set ratio"
    )
    parser.add_argument(
        "--test_ratio",
        type=float,
        default=0.1,
        help="Test set ratio"
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=42,
        help="Random seed"
    )
    
    args = parser.parse_args()
    
    # Set random seed
    random.seed(args.seed)
    
    print("\n" + "="*60)
    print("ğŸš€ Data Preparation Script")
    print("="*60)
    
    # Load dataset
    if args.dataset == "sample":
        data = create_sample_dataset(args.num_samples)
    elif args.dataset == "koalpaca":
        data = download_koalpaca_dataset(args.num_samples)
    elif args.dataset == "custom":
        if not args.custom_file:
            raise ValueError("--custom_file is required when dataset=custom")
        data = load_custom_dataset(args.custom_file)
    
    # Split dataset
    train_data, val_data, test_data = split_dataset(
        data,
        train_ratio=args.train_ratio,
        val_ratio=args.val_ratio,
        test_ratio=args.test_ratio
    )
    
    # Save datasets
    save_dataset(train_data, os.path.join(args.output_dir, "train.json"))
    save_dataset(val_data, os.path.join(args.output_dir, "validation.json"))
    save_dataset(test_data, os.path.join(args.output_dir, "test.json"))
    
    # Print sample
    print("\n" + "="*60)
    print("ğŸ“ Sample Data:")
    print("="*60)
    sample = train_data[0]
    print(f"Instruction: {sample['instruction']}")
    if sample['input']:
        print(f"Input: {sample['input']}")
    print(f"Output: {sample['output'][:100]}...")
    print("="*60)
    
    print("\nâœ… Data preparation complete!")
    print(f"ğŸ“ Output directory: {args.output_dir}")


if __name__ == "__main__":
    main()
